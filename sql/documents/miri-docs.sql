# -- Donee donation cases: MIRI

insert into documents(url,title,publication_date,modified_date,author,publisher,affected_donors,affected_donees,document_scope,cause_area,notes) values
  ('http://lesswrong.com/r/discussion/lw/mj0/miri_fundraiser_why_now_matters/','MIRI Fundraiser: Why now matters','2015-07-24',NULL,'Nate Soares','Machine Intelligence Research Institute',NULL,'Machine Intelligence Research Institute','Donee donation case','AI risk','Cross-posted at LessWrong and on the MIRI blog at https://intelligence.org/2015/07/20/why-now-matters/ -- this post occurs just two months after Soares takes over as MIRI Executive Director. It is a followup to https://intelligence.org/2015/07/17/miris-2015-summer-fundraiser/'),
  ('https://intelligence.org/2015/07/17/miris-2015-summer-fundraiser/','MIRI’s 2015 Summer Fundraiser!','2015-07-17',NULL,'Nate Soares','Machine Intelligence Research Institute',NULL,'Machine Intelligence Research Institute','Donee donation case','AI risk','MIRI announces its summer fundraiser and links to a number of documents to help donors evaluate it. This is the first fundraiser under new Executive Director Nate Soares, just a couple months after he assumed office'),
  ('https://intelligence.org/2016/09/16/miris-2016-fundraiser/','MIRI’s 2016 Fundraiser','2016-09-16',NULL,'Nate Soares','Machine Intelligence Research Institute',NULL,'Machine Intelligence Research Institute','Donee donation case','AI risk','MIRI announces its single 2016 fundraiser (as opposed to previous years when it conducted two fundraisers, it is conducting just one this time, in the Fall)'),
  ('https://www.facebook.com/robbensinger/posts/10157530911060447','Crunch time!! The 2016 fundraiser for the AI safety group I work at, MIRI, is going a lot slower than expected','2016-10-25','2016-10-25','Rob Bensinger','Facebook',NULL,'Machine Intelligence Research Institute','Donee donation case','AI risk','Rob Bensinger, Research Communications Director at MIRI, takes to his personal Facebook to ask people to chip in for the MIRI fundraiser, which is going slower than he and MIRI expected, and may not meet its target. The final comment by Bensinger notes that $582,316 out of the target of $750,000 was raised, and that about $260k of that was raised after his post, so he credits the final push for helping MIRI move closer to its fundraising goals'),
  ('https://intelligence.org/2017/12/01/miris-2017-fundraiser/','MIRI’s 2017 Fundraiser','2017-12-01',NULL,'Malo Bourgon','Machine Intelligence Research Institute',NULL,'Machine Intelligence Research Institute','Donee donation case','AI risk','Document provides cumulative target amounts for 2017 fundraiser ($625,000 Target 1, $850,000 Target 2, $1,250,000 Target 3) along with what MIRI expects to accomplish at each target level. Funds raised from the Open Philanthropy Project and an anonymous cryptocurrency donor (see https://intelligence.org/2017/07/04/updates-to-the-research-team-and-a-major-donation/ for more) are identified as reasons for the greater financial security and more long-term and ambitious planning'),
  ('https://intelligence.org/2017/12/14/end-of-the-year-matching/','End-of-the-year matching challenge!','2017-12-14',NULL,'Rob Bensinger','Machine Intelligence Research Institute','Christian Calderon|Marius van Voorden','Machine Intelligence Research Institute','Donee donation case','AI risk','MIRI gives an update on how its fundraising efforts are going, noting that it has met its first fundraising target, listing two major donations (Christian Calderon: $367,574 and Marius van Voorden: $59K), and highlighting the 2017 charity drive where donations up to $1 million to a list of charities including MIRI will be matched'),
  ('http://effective-altruism.com/ea/1io/miri_2017_fundraiser_and_strategy_update/','MIRI 2017 Fundraiser and Strategy Update','2017-12-15',NULL,'Malo Bourgon','Machine Intelligence Research Institute',NULL,'Machine Intelligence Research Institute','Donee donation case','AI risk','MIRI provides an update on its fundraiser and its strategy in a general-interest forum for people interested in effective altruism. They say the fundraiser is already going quite well, but believe they can still use marginal funds well to expand more');

# -- Other donee updates
insert into documents(url,title,publication_date,modified_date,author,publisher,affected_donors,affected_donees,document_scope,cause_area,notes) values
  ('https://intelligence.org/2017/04/30/2017-updates-and-strategy/','2017 Updates and Strategy','2017-04-30',NULL,'Rob Bensinger','Machine Intelligence Research Institute',NULL,'Machine Intelligence Research Institute','Donee periodic update','AI risk','MIRI provides updates on its progress as an organization and outlines its strategy and budget for the coming year. Key update is that recent developments in AI have made them increase the probability of AGI before 2035 by a little bit. MIRI has also been in touch with researchers at FAIR, DeepMind, and OpenAI'),
  ('https://intelligence.org/2017/07/04/updates-to-the-research-team-and-a-major-donation/','Updates to the research team, and a major donation','2017-07-04',NULL,'Malo Bourgon','Machine Intelligence Research Institute',NULL,'Machine Intelligence Research Institute','Donee periodic update','AI risk','MIRI announces a surprise $1.01 million donation from an Ethereum cryptocurrency investor (2017-05-30) as well as updates related to team and fundraising'),
  ('http://effective-altruism.com/ea/12r/ask_miri_anything_ama/','Ask MIRI Anything (AMA)','2016-10-11',NULL,'Rob Bensinger','Machine Intelligence Research Institute',NULL,'Machine Intelligence Research Institute','Donee AMA','AI risk','Rob Bensinger, the Research Communications Manager at MIRI, hosts an Ask Me Anything (AMA) on the Effective Altruism Forum during the October 2016 Fundraiser'),
  ('https://intelligence.org/2018/01/10/fundraising-success/','Fundraising success!','2018-01-10',NULL,'Malo Bourgon','Machine Intelligence Research Institute',NULL,'Machine Intelligence Research Institute','Donee periodic update','AI risk','MIRI announces the success of its fundraiser, providing information on its top doonors, and thanking everybody who contributed');

# -- External evaluations

insert into documents(url,title,publication_date,modified_date,author,publisher,affected_donors,affected_donees,document_scope,cause_area,notes) values
  ('http://lesswrong.com/lw/5il/siai_an_examination/','SIAI - An Examination','2011-05-02',NULL,'Brandon Reinhart','LessWrong','Brandon Reinhart','Machine Intelligence Research Institute','Evaluator review of donee','AI risk','Post discussing initial investigation into the Singularity Institute for Artificial Intelligence (SIAI), the former name of Machine Intelligence Research Institute (MIRI), with the intent of deciding whether to donate. Final takeaway is that it was a worthy donation target, though no specific donation is announced in the post. See http://lesswrong.com/r/discussion/lw/5fo/siai_fundraising/ for an earlier draft of the post (along with a number of comments that were incorporated into the official version)'),
  ('http://lesswrong.com/lw/cbs/thoughts_on_the_singularity_institute_si/','Thoughts on the Singularity Institute (SI)','2012-05-11',NULL,'Holden Karnofsky','LessWrong',NULL,'Machine Intelligence Research Institute','Evaluator review of donee','AI risk','Post discussing reasons Holden Karnofsky, co-executive director of GiveWell, does not recommend the Singularity Institute (SI), the historical name for the Machine Intelligence Research Institute'),
  ('http://slatestarcodex.com/2014/10/07/tumblr-on-miri/','Tumblr on MIRI','2014-10-07',NULL,'Scott Alexander','Slate Star Codex',NULL,'Machine Intelligence Research Institute','Evaluator review of donee','AI risk','The blog post is structured as a response to recent criticism of MIRI on Tumblr, but is mainly a guardedly positive assessment of MIRI. In particular, it highlights the important role played by MIRI in elevating the profile of AI risk, citing attention from Stephen Hawking, Elon Musk, Gary Drescher, Max Tegmark, Stuart Russell, and Peter Thiel.'),
  ('https://thingofthings.wordpress.com/2016/02/17/concerning-miris-place-in-the-ea-movement/','Concerning MIRI’s Place in the EA Movement','2016-02-17',NULL,'Ozy Brennan','Thing of Things',NULL,'Machine Intelligence Research Institute','Miscellaneous commentary','AI risk','The post does not directly evaluate MIRI, but highlights the importance of object-level evaluation of the quality and value of the work done by MIRI. Also thanks MIRI, LessWrong, and Yudkowsky for contributions to the growth of the effective altruist movement'),
  ('http://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/machine-intelligence-research-institute-general-support','Machine Intelligence Research Institute — General Support','2016-09-06',NULL,'Open Philanthropy Project','Open Philanthropy Project','Open Philanthropy Project','Machine Intelligence Research Institute','Evaluator review of donee','AI risk','Open Phil writes about the grant at considerable length, more than it usually does. This is because it says that it has found the investigation difficult and believes that others may benefit from its process. The writeup also links to reviews of MIRI research by AI researchers, commissioned by Open Phil: http://files.openphilanthropy.org/files/Grants/MIRI/consolidated_public_reviews.pdf (the reviews are anonymized). The date is based on the announcement date of the grant, see https://groups.google.com/a/openphilanthropy.org/forum/#!topic/newly.published/XkSl27jBDZ8 for the email'),  
  ('http://files.openphilanthropy.org/files/Grants/MIRI/consolidated_public_reviews.pdf','Anonymized Reviews of Three Recent Papers from MIRI’s Agent Foundations Research Agenda (PDF)','2016-09-06',NULL,NULL,'Open Philanthropy Project','Open Philanthropy Project','Machine Intelligence Research Institute','Evaluator review of donee','AI risk','Reviews of the technical work done by MIRI, solicited and compiled by the Open Philanthropy Project as part of its decision process behind a grant for general support to MIRI documented at http://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/machine-intelligence-research-institute-general-support (grant made 2016-08, announced 2016-09-06)'),
  ('https://www.facebook.com/vipulnaik.r/posts/10210792236177912','Belief status: off-the-cuff thoughts!','2017-01-19','2017-01-19','Vipul Naik','Facebook',NULL,'Machine Intelligence Research Institute','Reasoning supplement','AI risk','The post argues that (lack of) academic endorsement of the work done by MIRI should not be an important factor in evaluating MIRI, offering three reasons. Commenters include Rob Bensinger, Research Communications Manager at MIRI'),
  ('http://effective-altruism.com/ea/1ca/my_current_thoughts_on_miris_highly_reliable/','My current thoughts on MIRI’s highly reliable agent design work','2017-07-07',NULL,'Daniel Dewey','Effective Altruism Forum','Open Philanthropy Project','Machine Intelligence Research Institute','Evaluator review of donee','AI risk','Post discusses thoughts on the MIRI work on highly reliable agent design. Dewey is looking into the subject to inform Open Philanthropy Project grantmaking to MIRI specifically and for AI risk in general; the post reflects his own opinions that could affect Open Phil decisions. See https://groups.google.com/forum/#!topic/long-term-world-improvement/FeZ_h2HXJr0 for critical discussion, in particular the comments by Sarah Constantin'),
  ('https://www.facebook.com/robbensinger/posts/10159100119210447','I’ve noticed that this misconception is still floating around','2017-08-30',NULL,'Rob Bensinger','Facebook',NULL,'Machine Intelligence Research Institute','Reasoning supplement','AI risk','Post notes an alleged popular misconception that the reason to focus on AI risk is that it is low-probability but high-impact, but MIRI researchers assign a medium-to-high probability of AI risk in the medium-term future');

# -- Donors describe why they donate

insert into documents(url,title,publication_date,modified_date,author,publisher,affected_donors,affected_donees,document_scope,cause_area,notes) values
  ('http://effective-altruism.com/ea/14c/why_im_donating_to_miri_this_year/',"Why I'm donating to MIRI this year", #'
   '2016-11-30',NULL,'Owen Cotton-Barratt',NULL,'Owen Cotton-Barratt','Machine Intelligence Research Institute','Single donation documentation','AI risk','Primary interest is in existential risk. Cited CoI and other reasons for not donating to own employer, Centre for Effective Altruism. Notes disagreements with MIRI, citing http://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/machine-intelligence-research-institute-general-support#research but highlights need for epistemic humility'),
   ('https://thezvi.wordpress.com/2017/12/17/i-vouch-for-miri/','I Vouch For MIRI','2017-12-17',NULL,'Zvi Mowshowitz',NULL,'Zvi Mowshowitz','Machine Intelligence Research Institute','Single donation documentation','AI risk','Mowshowitz explains why he made his $10,000 donation to MIRI, and makes the case for others to support MIRI. He believes that MIRI understands the hardness of the AI safety problem, is focused on building solutions for the long term, and has done humanity a great service through its work on functional decision theory'),
   ('https://putanumonit.com/2017/12/10/worried-about-ai/','AI: a Reason to Worry, and to Donate','2017-12-10',NULL,'Jacob Falkovich',NULL,'Jacob Falkovich','Machine Intelligence Research Institute|Future of Life Institute|Center for Human-Compatible AI|Berkeley Existential Risk Initiative|Future of Humanity Institute|Effective Altruism Funds','Single donation documentation','AI risk','Falkovich explains why he thinks AI safety is a much more important and relatively neglected existential risk than climate change, and why he is donating to it. He says he is donating to MIRI because he is reasonably certain of the importance of their work on AI aligment. However, he lists a few other organizations for which he is willing to match donations up to 0.3 bitcoins, and encourages other donors to use their own judgment to deicde among them: Future of Life Institute, Center for Human-Compatible AI, Berkeley Existential Risk Initiative, Future of Humanity Institute, and Effective Altruism Funds (the Long-Term Future Fund)');
   

