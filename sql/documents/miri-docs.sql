# -- Donee donation cases: MIRI

insert into documents(url,title,publication_date,modified_date,author,publisher,affected_donors,affected_donees,document_scope,cause_area,notes) values
  ('http://lesswrong.com/r/discussion/lw/mj0/miri_fundraiser_why_now_matters/','MIRI Fundraiser: Why now matters','2015-07-24',NULL,'Nate Soares','Machine Intelligence Research Institute',NULL,'Machine Intelligence Research Institute','Donee donation case','AI risk','Cross-posted at LessWrong and on the MIRI blog at https://intelligence.org/2015/07/20/why-now-matters/ -- this post occurs just two months after Soares takes over as MIRI Executive Director. It is a followup to https://intelligence.org/2015/07/17/miris-2015-summer-fundraiser/'),
  ('https://intelligence.org/2015/07/17/miris-2015-summer-fundraiser/','MIRI’s 2015 Summer Fundraiser!','2015-07-17',NULL,'Nate Soares','Machine Intelligence Research Institute',NULL,'Machine Intelligence Research Institute','Donee donation case','AI risk','MIRI announces its summer fundraiser and links to a number of documents to help donors evaluate it. This is the first fundraiser under new Executive Director Nate Soares, just a couple months after he assumed office'),
  ('https://intelligence.org/2016/09/16/miris-2016-fundraiser/','MIRI’s 2016 Fundraiser','2016-09-16',NULL,'Nate Soares','Machine Intelligence Research Institute',NULL,'Machine Intelligence Research Institute','Donee donation case','AI risk','MIRI announces its single 2016 fundraiser (as opposed to previous years when it conducted two fundraisers, it is conducting just one this time, in the Fall)');

# -- Other donee updates
insert into documents(url,title,publication_date,modified_date,author,publisher,affected_donors,affected_donees,document_scope,cause_area,notes) values
  ('https://intelligence.org/2017/04/30/2017-updates-and-strategy/','2017 Updates and Strategy','2017-04-30',NULL,'Rob Bensinger','Machine Intelligence Research Institute',NULL,'Machine Intelligence Research Institute','Donee periodic update','AI risk','MIRI provides updates on its progress as an organization and outlines its strategy and budget for the coming year. Key update is that recent developments in AI have made them increase the probability of AGI before 2035 by a little bit. MIRI has also been in touch with researchers at FAIR, DeepMind, and OpenAI'),
  ('https://intelligence.org/2017/07/04/updates-to-the-research-team-and-a-major-donation/','Updates to the research team, and a major donation','2017-07-04',NULL,'Malo Bourgon','Machine Intelligence Research Institute',NULL,'Machine Intelligence Research Institute','Donee periodic update','AI risk','MIRI announces a surprise $1.01 million donation from an Ethereum cryptocurrency investor (2017-05-30) as well as updates related to team and fundraising');

# -- External evaluations

insert into documents(url,title,publication_date,modified_date,author,publisher,affected_donors,affected_donees,document_scope,cause_area,notes) values
  ('http://lesswrong.com/lw/cbs/thoughts_on_the_singularity_institute_si/','Thoughts on the Singularity Institute (SI)','2012-05-11',NULL,'Holden Karnofsky','LessWrong',NULL,'Machine Intelligence Research Institute','Evaluator review of donee','AI risk','Post discussing reasons Holden Karnofsky, co-executive director of GiveWell, does not recommend the Singularity Institute (SI), the historical name for the Machine Intelligence Research Institute'),
  ('http://lesswrong.com/lw/5il/siai_an_examination/','SIAI - An Examination','2011-05-02',NULL,'Brandon Reinhart','LessWrong','Brandon Reinhart','Machine Intelligence Research Institute','Evaluator review of donee','AI risk','Post discussing initial investigation into the Singularity Institute for Artificial Intelligence (SIAI), the former name of Machine Intelligence Research Institute (MIRI), with the intent of deciding whether to donate. Final takeaway is that it was a worthy donation target, though no specific donation is announced in the post'),
  ('http://files.openphilanthropy.org/files/Grants/MIRI/consolidated_public_reviews.pdf','Anonymized Reviews of Three Recent Papers from MIRI’s Agent Foundations Research Agenda (PDF)','2016-09-06',NULL,NULL,'Open Philanthropy Project','Open Philanthropy Project','Machine Intelligence Research Institute','Evaluator review of donee','AI risk','Reviews of the technical work done by MIRI, solicited and compiled by the Open Philanthropy Project as part of its decision process behind a grant for general support to MIRI documented at http://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/machine-intelligence-research-institute-general-support (grant made 2016-08, announced 2016-09-06)'),
  ('http://effective-altruism.com/ea/1ca/my_current_thoughts_on_miris_highly_reliable/','My current thoughts on MIRI’s highly reliable agent design work','2017-07-07',NULL,'Daniel Dewey','Effective Altruism Forum','Open Philanthropy Project','Machine Intelligence Research Institute','Evaluator review of donee','AI risk','Post discusses thoughts on the MIRI work on highly reliable agent design');

