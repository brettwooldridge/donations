# -- AI safety general

insert into documents(url,title,publication_date,modified_date,author,publisher,affected_donors,affected_donees,document_scope,cause_area,notes) values
  ('http://effective-altruism.com/ea/14w/2017_ai_risk_literature_review_and_charity/','2017 AI Risk Literature Review and Charity Comparison','2016-12-13',NULL,'Ben Hoskin','Effective Altruism Forum','Ben Hoskin','Machine Intelligence Research Institute|Future of Humanity Institute|OpenAI|Center for Human-Compatible AI|Future of Life Institute|Centre for the Study of Existential Risk|Leverhulme Centre for the Future of Intelligence|Global Catastrophic Risk Institute|Global Priorities Project|AI Impacts|Xrisks Institute|X-Risks Net|Center for Applied Rationality|80,000 Hours|Raising for Effective Giving','Review of current state of cause area','AI safety','The lengthy blog post covers all the published work of prominent organizations focused on AI risk. References http://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/machine-intelligence-research-institute-general-support#sources1007 for the MIRI part of it but notes the absence of information on the many other orgs. The conclusion: "The conclusion: "Significant donations to the Machine Intelligence Research Institute and the Global Catastrophic Risks Institute. A much smaller one to AI Impacts."'),
  ('https://www.centreforeffectivealtruism.org/blog/changes-in-funding-in-the-ai-safety-field','Changes in funding in the AI safety field','2017-02-01','2017-02-02','Sebastian Farquhar','Centre for Effective Altruism',NULL,'Machine Intelligence Research Institute|Center for Human-Compatible AI|Leverhulme Centre for the Future of Intelligence|Future of Life Institute|Future of Humanity Institute|OpenAI|MIT Media Lab','Review of current state of cause area','AI safety','The post reviews AI safety funding from 2014 to 2017 (projections for 2017). Cross-posted on EA Forum at http://effective-altruism.com/ea/16s/changes_in_funding_in_the_ai_safety_field/'),
  ('http://effective-altruism.com/ea/1iu/2018_ai_safety_literature_review_and_charity/','2018 AI Safety Literature Review and Charity Comparison','2017-12-20',NULL,'Ben Hoskin','Effective Altruism Forum','Ben Hoskin','Machine Intelligence Research Institute|Future of Humanity Institute|Global Catastrophic Risk Institute|Centre for the Study of Existential Risk|AI Impacts|Center for Human-Compatible AI|Center for Applied Rationality|Future of Life Institute|80,000 Hours','Review of current state of cause area','AI safety','The lengthy blog post covers all the published work of prominent organizations focused on AI risk. It is an annual refresh of http://effective-altruism.com/ea/14w/2017_ai_risk_literature_review_and_charity/ -- a similar post published a year before it. The conclusion: "Significant donations to the Machine Intelligence Research Institute and the Global Catastrophic Risks Institute. A much smaller one to AI Impacts."'),
  ('https://www.lesswrong.com/posts/XFpDTCHZZ4wpMT8PZ/a-model-i-use-when-making-plans-to-reduce-ai-x-risk','A model I use when making plans to reduce AI x-risk','2018-01-18',NULL,'Ben Pace','LessWrong','Berkeley Existential Risk Initiative',NULL,'Review of current state of cause area','AI safety','The author describes his implicit model of AI risk, with four parts: (1) Alignment is hard, (2) Getting alignment right accounts for most of the variance in whether an AGI system will be positive for humanity, (3) Our current epistemic state regarding AGI timelines will continue until we are close (<2 years from) to having AGI, and (4) Given timeline uncertainty, it is best to spend marginal effort on plans that assume / work in shorter timelines. There is a lot of discussion in the comments'),
  ('http://effective-altruism.com/ea/1lq/opportunities_for_individual_donors_in_ai_safety/','Opportunities for individual donors in AI safety','2018-03-12',NULL,'Alex Flint','Effective Altruism Forum',NULL,'Machine Intelligence Research Institute|Future of Humanity Institute','Review of current state of cause area','AI safety','Alex Flint discusses the history of AI safety funding, and suggests some heuristics for individual donors based on what he has seen to be successful in the past.');
