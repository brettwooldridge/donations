# -- AI safety general

insert into documents(url,title,publication_date,modified_date,author,publisher,affected_donors,affected_donees,document_scope,cause_area,notes) values
  ('https://forum.effectivealtruism.org/posts/nSot23sAjoZRgaEwa/2016-ai-risk-literature-review-and-charity-comparison','2016 AI Risk Literature Review and Charity Comparison','2016-12-13',NULL,'Ben Hoskin','Effective Altruism Forum','Ben Hoskin','Machine Intelligence Research Institute|Future of Humanity Institute|OpenAI|Center for Human-Compatible AI|Future of Life Institute|Centre for the Study of Existential Risk|Leverhulme Centre for the Future of Intelligence|Global Catastrophic Risk Institute|Global Priorities Project|AI Impacts|Xrisks Institute|X-Risks Net|Center for Applied Rationality|80,000 Hours|Raising for Effective Giving','Review of current state of cause area','AI safety','The lengthy blog post covers all the published work of prominent organizations focused on AI risk. References https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/machine-intelligence-research-institute-general-support#sources1007 for the MIRI part of it but notes the absence of information on the many other orgs. The conclusion: "The conclusion: "Donate to both the Machine Intelligence Research Institute and the Future of Humanity Institute, but somewhat biased towards the former. I will also make a smaller donation to the Global Catastrophic Risks Institute."'),
  ('https://www.centreforeffectivealtruism.org/blog/changes-in-funding-in-the-ai-safety-field','Changes in funding in the AI safety field','2017-02-01','2017-02-02','Sebastian Farquhar','Centre for Effective Altruism',NULL,'Machine Intelligence Research Institute|Center for Human-Compatible AI|Leverhulme Centre for the Future of Intelligence|Future of Life Institute|Future of Humanity Institute|OpenAI|MIT Media Lab','Review of current state of cause area','AI safety','The post reviews AI safety funding from 2014 to 2017 (projections for 2017). Cross-posted on EA Forum at http://effective-altruism.com/ea/16s/changes_in_funding_in_the_ai_safety_field/'),
  ('https://forum.effectivealtruism.org/posts/XKwiEpWRdfWo7jy7f/2017-ai-safety-literature-review-and-charity-comparison','2017 AI Safety Literature Review and Charity Comparison','2017-12-20',NULL,'Ben Hoskin','Effective Altruism Forum','Ben Hoskin','Machine Intelligence Research Institute|Future of Humanity Institute|Global Catastrophic Risk Institute|Centre for the Study of Existential Risk|AI Impacts|Center for Human-Compatible AI|Center for Applied Rationality|Future of Life Institute|80,000 Hours','Review of current state of cause area','AI safety','The lengthy blog post covers all the published work of prominent organizations focused on AI risk. It is an annual refresh of https://forum.effectivealtruism.org/posts/nSot23sAjoZRgaEwa/2016-ai-risk-literature-review-and-charity-comparison -- a similar post published a year before it. The conclusion: "Significant donations to the Machine Intelligence Research Institute and the Global Catastrophic Risks Institute. A much smaller one to AI Impacts."'),
  ('https://www.lesswrong.com/posts/XFpDTCHZZ4wpMT8PZ/a-model-i-use-when-making-plans-to-reduce-ai-x-risk','A model I use when making plans to reduce AI x-risk','2018-01-18',NULL,'Ben Pace','LessWrong','Berkeley Existential Risk Initiative',NULL,'Review of current state of cause area','AI safety','The author describes his implicit model of AI risk, with four parts: (1) Alignment is hard, (2) Getting alignment right accounts for most of the variance in whether an AGI system will be positive for humanity, (3) Our current epistemic state regarding AGI timelines will continue until we are close (<2 years from) to having AGI, and (4) Given timeline uncertainty, it is best to spend marginal effort on plans that assume / work in shorter timelines. There is a lot of discussion in the comments'),
  ('https://forum.effectivealtruism.org/posts/HqatEhdEb42vhSo7B/opportunities-for-individual-donors-in-ai-safety','Opportunities for individual donors in AI safety','2018-03-12',NULL,'Alex Flint','Effective Altruism Forum',NULL,'Machine Intelligence Research Institute|Future of Humanity Institute','Review of current state of cause area','AI safety','Alex Flint discusses the history of AI safety funding, and suggests some heuristics for individual donors based on what he has seen to be successful in the past.'),
  ('https://forum.effectivealtruism.org/posts/BznrRBgiDdcTwWWsB/2018-ai-alignment-literature-review-and-charity-comparison','2018 AI Alignment Literature Review and Charity Comparison','2018-12-17',NULL,'Ben Hoskin','Effective Altruism Forum','Ben Hoskin','Machine Intelligence Research Institute|Future of Humanity Institute|Center for Human-Compatible AI|Centre for the Study of Existential Risk|Global Catastrophic Risk Institute|Global Priorities Institute|Australian National University|Berkeley Existential Risk Initiative|Ought|AI Impacts|OpenAI|Effective Altruism Foundation|Foundational Research Institute|Median Group|Convergence Analysis','Review of current state of cause area','AI safety','Cross-posted to LessWrong at https://www.lesswrong.com/posts/a72owS5hz3acBK5xc/2018-ai-alignment-literature-review-and-charity-comparison This is the third post in a tradition of annual blog posts on the state of AI safety and the work of various organizations in the space over the course of the year; the previous two blog posts are at https://forum.effectivealtruism.org/posts/nSot23sAjoZRgaEwa/2016-ai-risk-literature-review-and-charity-comparison and https://forum.effectivealtruism.org/posts/XKwiEpWRdfWo7jy7f/2017-ai-safety-literature-review-and-charity-comparison The post has a "methodological considerations" section that discusses how the author views track records, politics, openness, the research flywheel, near vs far safety research, other existential risks, financial reserves, donation matching, poor quality research, and the Bay Area. The number of organizations reviewed is also larger than in previous years. Excerpts from the conclusion: "Despite having donated to MIRI consistently for many years as a result of their highly non-replaceable and groundbreaking work in the field, I cannot in good faith do so this year given their lack of disclosure. [...] This is the first year I have attempted to review CHAI in detail and I have been impressed with the quality and volume of their work. I also think they have more room for funding than FHI. As such I will be donating some money to CHAI this year. [...] As such I will be donating some money to GCRI again this year. [...] As such I do not plan to donate to AI Impacts this year, but if they are able to scale effectively I might well do so in 2019. [...] I also plan to start making donations to individual researchers, on a retrospective basis, for doing useful work. [...] This would be somewhat similar to Impact Certificates, while hopefully avoiding some of their issues.');

