# -- AI safety donations
insert into donations(donor, donee, amount, donation_date, donation_date_precision, donation_date_basis, cause_area, url, donor_cause_area_url, notes, donation_earmark, influencer, donation_announcement_date, donation_announcement_date_precision,donation_announcement_url, predictions) values
  ('Open Philanthropy Project','Future of Life Institute',1186000,'2015-08-01','month','donation log','AI safety','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/future-life-institute-artificial-intelligence-risk-reduction','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence','Grant accompanied a grant by Elon Musk to FLI for the same purpose. See also the March 2015 blog post https://www.openphilanthropy.org/blog/open-philanthropy-project-update-global-catastrophic-risks that describes strategy and developments prior to the grant. An update on the grant was posted in 2017-04 at https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/update-fli-grant discussing impressions of Howie Lempel and Daniel Dewey of the grant and of the effect on and role of Open Phil', NULL, NULL,'2015-08-26','day','https://groups.google.com/a/openphilanthropy.org/forum/#!topic/newly.published/q5PuecwdXGk',NULL),
  ('Open Philanthropy Project','Electronic Frontier Foundation',199000,'2016-11-01','month','donation log','AI safety','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/electronic-frontier-foundation-ai-social','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence','Grant funded work by Peter Eckersley, whom the Open Philanthropy Project believed in. Followup conversation with Peter Eckersley and Jeremy Gillula of grantee organization at https://www.openphilanthropy.org/sites/default/files/Peter_Eckersley_Jeremy_Gillula_05-26-16_%28public%29.pdf on 2016-05-26', 'Peter Eckersley', NULL, '2016-12-15','day','https://groups.google.com/a/openphilanthropy.org/forum/#!topic/newly.published/qVU8m1QRQPI',NULL),
  ('Open Philanthropy Project','Machine Intelligence Research Institute',500000,'2016-08-01','month','donation log','AI safety','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/machine-intelligence-research-institute-general-support','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence','The grant page describes grant process in detail; also links to commissioned reviews at http://files.openphilanthropy.org/files/Grants/MIRI/consolidated_public_reviews.pdf The grant was also announced on the MIRI website at https://intelligence.org/2016/08/05/miri-strategy-update-2016/', NULL, NULL, '2016-09-06', 'day', 'https://groups.google.com/a/openphilanthropy.org/forum/#!topic/newly.published/XkSl27jBDZ8',NULL),
  ('Open Philanthropy Project','George Mason University',277435,'2016-06-01','month','donation log','AI safety','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/george-mason-university-research-future-artificial-intelligence-scenarios','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence','Earmarked for Robin Hanson research. Grant page references https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence for background. Original amount $264,525. Increased to $277,435 through the addition of $12,910 in July 2017 to cover an increase in George Mason University’s instructional release costs (teaching buyouts).', 'Robin Hanson', NULL, '2016-07-07', 'day', 'https://groups.google.com/a/openphilanthropy.org/forum/#!topic/newly.published/ZDGCJGCb2C4',NULL),
  ('Open Philanthropy Project','Center for Human-Compatible AI',5555550,'2016-08-01','month','donation log','AI safety','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-berkeley-center-human-compatible-ai','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence','Grant page references https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence and https://www.openphilanthropy.org/blog/potential-risks-advanced-artificial-intelligence-philanthropic-opportunity and states a 50% chance that two years from the grant date, the Center will be spending at least $2 million a year, and will be considered by one or more of our relevant technical advisors to have a reasonably good reputation in the field. Note that the grant recipient in the Open Phil database has been listed as UC Berkeley, but we have written it as the naem of the center for easier cross-referencing', NULL, NULL, '2016-08-29', 'day', 'https://groups.google.com/a/openphilanthropy.org/forum/#!topic/newly.published/3ouYTW5lXBY','https://predictionbook.com/predictions/185224'),
  ('Open Philanthropy Project','AI Impacts',32000,'2016-12-01','month','donation log','AI safety','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/ai-impacts-general-support','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence','Grant for work on strategic questions related to potential risks from advanced artificial intelligence.', NULL, NULL, '2017-02-02', 'day', 'https://groups.google.com/a/openphilanthropy.org/forum/#!topic/newly.published/FU2Is9LMWJg',NULL),
  ('Open Philanthropy Project','Future of Humanity Institute',1994000,'2017-03-01','month','donation log','AI safety','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/future-humanity-institute-general-support','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence','Grant for general support. A related grant specifically for biosecurity work was granted in 2016-09, made earlier for logistical reasons', NULL, NULL, '2017-03-06', 'day', 'https://groups.google.com/a/openphilanthropy.org/forum/#!topic/newly.published/Ehx9XD2XYPg',NULL),
  ('Open Philanthropy Project','OpenAI',30000000,'2017-03-01','month','donation log','AI safety','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/openai-general-support','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence','Grant for general support; 10 million dollars each over three years. Grant write-up explains reasons for this grant being larger than other grants in the cause area. External discussions include https://twitter.com/Pinboard/status/848009582492360704 (critical tweet with replies), https://www.facebook.com/vipulnaik.r/posts/10211478311489366 (Facebook post by Vipul Naik, with some comments), https://www.facebook.com/groups/effective.altruists/permalink/1350683924987961/ (Facebook post by Alasdair Pearce in Effective Altruists Facebook group, with some comments), and https://news.ycombinator.com/item?id=14008569 (Hacker News post, with some comments)',NULL, NULL, '2017-03-31', 'day', 'https://groups.google.com/a/openphilanthropy.org/forum/#!topic/newly.published/PqgSlJy0JT4',NULL),
  ('Open Philanthropy Project','Montreal Institute for Learning Algorithms',2400000,'2017-07-01','month','donation log','AI safety','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/montreal-institute-learning-algorithms-ai-safety-research','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence','Grant to support research to improve the positive long-term impact of artificial intelligence on society. Mainly due to star power of researcher Yoshua Bengio who influences many young ML/AI researchers. Detailed writeup available. See also https://www.facebook.com/permalink.php?story_fbid=10110258359382500&id=13963931 for a Facebook share by David Krueger, a member of the grantee organization. The comments include some discussion about the grantee','Yoshua Bengio', NULL, '2017-07-19','day','https://groups.google.com/a/openphilanthropy.org/forum/#!topic/newly.published/qL9PbCP-U3w',NULL),
  ('Open Philanthropy Project','UCLA School of Law',1536222,'2017-05-01','month','donation log','AI safety','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/ucla-artificial-intelligence-governance','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence','Grant to support work on governance related to AI risk led by Edward Parson and Richard Re','Edward Parson,Richard Re','Helen Toner','2017-07-27','day','https://groups.google.com/a/openphilanthropy.org/forum/#!topic/newly.published/hCmf-IGe40Y',NULL),
  ('Open Philanthropy Project','Distill',25000,'2017-03-01','month','donation log','AI safety','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/distill-prize-clarity-machine-learning-general-support','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence','Grant covers 25000 out of a total of 125000 USD initial endowment for the Distill prize https://distill.pub/prize/ administered by the Open Philanthropy Project. Other contributors to the endowment include Chris Olah, Greg Brockman, Jeff Dean, and DeepMind. The Open Philanthropy Project grant page says: "Without our funding, we estimate that there is a 60% chance that the prize would be administered at the same level of quality, a 30% chance that it would be administered at lower quality, and a 10% chance that it would not move forward at all. We believe that our assistance in administering the prize will also be of significant help to Distill."',NULL,'Daniel Dewey','2017-08-11','day','https://groups.google.com/a/openphilanthropy.org/forum/#!topic/newly.published/iyErXoVcmSE',NULL),
  ('Open Philanthropy Project','Stanford University',25000,'2017-03-01','month','donation log','AI safety','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/stanford-university-percy-liang-planning-grant','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence','Grant awarded to Professor Percy Liang to spend significant time engaging in the Open Philanthropy Project grant application process, that would lead to a larger grant https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/stanford-university-support-percy-liang of $1,337,600','Percy Liang','Daniel Dewey','2017-09-26','day','https://groups.google.com/a/openphilanthropy.org/forum/#!topic/newly.published/2zSvBZmn_J8',NULL),
  ('Open Philanthropy Project','Stanford University',1337600,'2017-05-01','month','donation log','AI safety','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/stanford-university-support-percy-liang','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence','Grant awarded over year years (July 2017 to July 2021) to support research by Professor Percy Liang and three graduate students on AI safety and alignment. The funds will be split approximately evenly across the four years (i.e. roughly $320,000 to $350,000 per year). Preceded by planning grant https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/stanford-university-support-percy-liang of $25,000','Percy Liang','Daniel Dewey','2017-09-26','day','https://groups.google.com/a/openphilanthropy.org/forum/#!topic/newly.published/2zSvBZmn_J8',NULL),
  ('Open Philanthropy Project','Berkeley Existential Risk Initiative',403890,'2017-07-01','month','donation log','AI safety','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/berkeley-existential-risk-initiative-core-staff-and-chai-collaboration','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence','Grant to support core functions of grantee, and to help them provide contract workers for the Center for Human-Compatible AI (CHAI) housed at the University of California, Berkeley, also an Open Phil grantee (see https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-berkeley-center-human-compatible-ai for info on that grant). Open Phil also sees this as a promising model for providing assistance to other BERI clients in the future',NULL,'Daniel Dewey','2017-09-28','day','https://groups.google.com/a/openphilanthropy.org/forum/#!topic/newly.published/5UjOZN6KlWQ',NULL),
  ('Open Philanthropy Project','Yale University',299320,'2017-07-01','month','donation log','AI safety','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/yale-university-global-politics-of-ai-dafoe','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence','Grant to support research into the global politics of artificial intelligence, led by Assistant Professor of Political Science, Allan Dafoe, who will conduct part of the research at the Future of Humanity Institute in Oxford, United Kingdom over the next year. Funds from the two gifts will support the hiring of two full-time research assistants, travel, conferences, and other expenses related to the research efforts, as well as salary, relocation, and health insurance expenses related to Professor Dafoe’s work in Oxford.','Allan Dafoe','Nick Beckstead','2017-09-28','day','https://groups.google.com/a/openphilanthropy.org/forum/#!topic/newly.published/5UjOZN6KlWQ',NULL),
  ('Open Philanthropy Project','University of California, Berkeley',1450016,'2017-10-01','month','donation log','AI safety','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-berkeley-ai-safety-levine-dragan','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence','Pair of grants to support AI safety work led by Professors Sergey Levine and Anca Dragan, who would each devote half their time to the project, with additional assistance from four graduate students. They initially intend to focus their research on how objective misspecification can produce subtle or overt undesirable behavior in robotic systems','Sergey Levine,Anca Dragan','Daniel Dewey','2017-10-20','day','https://groups.google.com/a/openphilanthropy.org/forum/#!topic/newly.published/jW3KjxVUOOA',NULL),
  ('Open Philanthropy Project','Machine Intelligence Research Institute',3750000,'2017-10-01','month','donation log','AI safety','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/machine-intelligence-research-institute-general-support-2017','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence','Grant over three years for general support. Represents a renewal and increase of the previous $500,000 grant in 2016 announced at https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/machine-intelligence-research-institute-general-support -- the increase is due to a positive review by a leading researcher of the logical induction work by MIRI, and a general increase in Open Phil grants in AI risk making it less likely that a large grant to MIRI will be construed as too much of an endorsement of their approach. Donee (MIRI) also blogged this grant at https://intelligence.org/2017/11/08/major-grant-open-phil/',NULL,'Nick Beckstead','2017-11-08','day','https://groups.google.com/a/openphilanthropy.org/forum/#!msg/newly.published/sym4vNvqFbw/d3CCxBZ1BgAJ',NULL),
  ('Open Philanthropy Project','Stanford University',2539,'2018-04-01','month','donation log','AI safety','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/stanford-nips-workshop-machine-learning','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence','Discretionary grant to support the Neural Information Processing System (NIPS) workshop “Machine Learning and Computer Security.” at https://nips.cc/Conferences/2017/Schedule?showEvent=8775',NULL,'Daniel Dewey','2018-04-19','day',NULL,NULL),
  ('Open Philanthropy Project','Ought',525000,'2018-05-01','month','donation log','AI safety','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/ought-general-support','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence','Grantee has a mission to “leverage machine learning to help people think.” Ought plans to conduct research on deliberation and amplification, a concept we consider relevant to AI alignment. The funding, combined with another grant from Open Philanthropy Project technical advisor Paul Christiano, is intended to allow Ought to hire up to three new staff members and provide one to three years of support for Ought’s work, depending how quickly they hire.',NULL,'Daniel Dewey','2018-05-31','day',NULL,NULL),
  ('Open Philanthropy Project','AI Fellows Program',1135000,'2018-05-01','month','donation log','AI safety','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/ai-fellows-program-2018','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence','Grants to 7 AI Fellows pursuing research relevant to AI risk. More details at https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence','Aditi Raghunathan|Chris Maddison|Felix Berkenkamp|Jon Gauthier|Michael Janner|Noam Brown|Ruth Fong','Daniel Dewey','2018-06-01','day',NULL,NULL),
  ('Open Philanthropy Project','AI Impacts',100000,'2018-06-01','month','donation log','AI safety','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/ai-impacts-general-support-2018','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence','Discretionary grant via the Machine Intelligence Research Institute. AI Impacts plans to use this grant to work on strategic questions related to potential risks from advanced artificial intelligence.. Renewal of December 2016 grant: https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/ai-impacts-general-support',NULL,'Daniel Dewey','2018-06-28','day',NULL,NULL),
  ('Open Philanthropy Project','Machine Intelligence Research Institute',150000,'2018-06-01','month','donation log','AI safety','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/machine-intelligence-research-institute-ai-safety-retraining-program','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence','Grant to suppport the artificial intelligence safety retraining project. MIRI intends to use these funds to provide stipends, structure, and guidance to promising computer programmers and other technically proficient individuals who are considering transitioning their careers to focus on potential risks from advanced artificial intelligence.',NULL,'Claire Zabel','2018-06-28','day',NULL,NULL),
  ('Open Philanthropy Project','University of Oxford',429770,'2018-07-01','month','donation log','AI safety','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/oxford-university-global-politics-of-ai-dafoe','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence','Grant to support research on the global politics of advanced artificial intelligence. The work will be led by Professor Allan Dafoe at the Future of Humanity Institute in Oxford, United Kingdom. The Open Philanthropy Project recommended additional funds to support this work in 2017, while Professor Dafoe was at Yale. Continuation of grant https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/yale-university-global-politics-of-ai-dafoe','Allan Dafoe','Nick Beckstead','2018-07-20','day',NULL,NULL),
  ('Open Philanthropy Project','AI Scholarships',159000,'2018-02-01','month','donation log','AI safety','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/ai-scholarships-2018','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence','Discretionary grant; total across grants to two artificial intelligence researcher, both over two years. The funding is intended to be used for the students’ tuition, fees, living expenses, and travel during their respective degree programs, and is part of an overall effort to grow the field of technical AI safety by supporting value-aligned and qualified early-career researchers. Recipients are Dmitrii Krasheninnikov, master’s degree, University of Amsterdam and Michael Cohen, master’s degree, Australian National University','Dmitrii Krasheninnikov|Michael Cohen','Daniel Dewey','2018-07-26','day',NULL,NULL),
  ('Open Philanthropy Project','The Wilson Center',400000,'2018-07-01','month','donation log','AI safety','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/wilson-center-ai-policy-seminar-series','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence','Grant over two years to support a series of in-depth AI policy seminars. Named for President Woodrow Wilson, the Wilson Center is a non-partisan policy forum for tackling global issues through independent research and open dialogue. Open Phil believes the seminar series could help raise the salience of AI policy in Washington, D.C. policymaking circles, and could help us identify and empower one or more influential thinkers in those circles, a key component of the Open Phil AI policy strategy.',NULL,'Luke Muehlhauser','2018-08-02','day',NULL,NULL),
  ('Open Philanthropy Project','Stanford University',100000,'2018-07-01','month','donation log','AI safety','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/stanford-university-machine-learning-security-research-dan-boneh-florian-tramer','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence','Grant is a "gift" to Stanford Unviersity to support machine learning security research led by Professor Dan Boneh and his PhD student, Florian Tramer. Machine learning security probes worst-case performance of learned models. Believed to be a way of pushing in the direction of more AI safety concern in machine learning research and AI development','Dan Boneh|Florian Tremer','Daniel Dewey','2018-09-07','day',NULL,NULL),
  ('Open Philanthropy Project','GoalsRL',7500,'2018-08-01','month','donation log','AI safety','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/goals-rl-workshop-on-goal-specifications-for-reinforcement-learning','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence','Discretionary grant to offset travel, registration, and other expenses associated with attending the GoalsRL 2018 workshop on goal specifications for reinforcement learning. The workshop was organized by Ashley Edwards, a recent computer science PhD candidate interested in reward learning.','Ashley Edwards','Daniel Dewey','2018-10-05','day',NULL,NULL),
  ('Open Philanthropy Project','University of California, Berkeley',1145000,'2018-11-01','month','donation log','AI safety','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/university-of-california-berkeley-artificial-intelligence-safety-research-2018','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence','Total across two grants over three years for machine learning researchers Pieter Abbeel and Aviv Tamar to study uses of generative models for robustness and interpretability. This funding will allow Mr. Abbeel and Mr. Tamar to fund PhD students and summer undergraduates to work on classifiers, imitation learning systems, and reinforcement learning systems.','Pieter Abeel|Aviv Tamar','Daniel Dewey','2018-12-12','day',NULL,NULL),
  ('Open Philanthropy Project','Berkeley Existential Risk Initiative',250000,'2019-01-01','month','donation log','AI safety','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/berkeley-existential-risk-initiative-chai-ml-engineers','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence','Grant to BERI to temporarily or permanently hire machine learning research engineers dedicated to BERI’s collaboration with the Center for Human-compatible Artificial Intelligence (CHAI). Follows previous support https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-berkeley-center-human-compatible-ai for the launch of CHAI and previous grant https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/berkeley-existential-risk-initiative-core-staff-and-chai-collaboration to collaborate with CHAI',NULL,'Daniel Dewey','2019-03-05','day',NULL,NULL),
  ('Open Philanthropy Project','Machine Intelligence Research Institute',2112500,'2019-02-01','month','donation log','AI safety','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/machine-intelligence-research-institute-general-support-2019','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence','Grant over two years for general support, supplementing the three-year October 2017 support https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/machine-intelligence-research-institute-general-support-2017 The exact grant amount was determined by the aggregated judgments of the Committee for Effective Altruism Support https://www.openphilanthropy.org/committee-effective-altruism-support The grant is also acknowledged on the grantee website https://intelligence.org/2019/04/01/new-grants-open-phil-beri/ which further says that the grant amount will be split equally between the years, and does the math to show that the total support from Open Phil to MIRI in 2019 will be $1.25 million (from the 2017 3-year grant of $3.75 million) + $1.06 million (half of the new two-year grant) or $2.31 million. The blog post further clarifies that the grant puts a lower bound on support in 2020 of ~$1.06 million (half of the new two-year grant) but that Open Phil may grant more money for 2020',NULL,'Claire Zabel|Committee for Effective Altruism Support','2019-04-02','day',NULL,NULL);
