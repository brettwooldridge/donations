# -- Global catastrophic risks/long-term future (grants made from the Long Term Future Fund. Some of them may be in other cause areas though)
insert into donations(donor, donee, amount, donation_date, donation_date_precision, donation_date_basis, cause_area, url, donor_cause_area_url, influencer, goal_amount, fraction, notes, affected_countries, donation_earmark) values
  ('Effective Altruism Funds: Long-Term Future Fund','Berkeley Existential Risk Initiative',14838.02,'2017-04-01','quarter','donation log','AI safety/other global catastrophic risks','https://app.effectivealtruism.org/funds/far-future/payouts/OzIQqsVacUKw0kEuaUGgI','https://app.effectivealtruism.org/funds/far-future','Nick Beckstead',14838.02,1.0,'Grant discussed at http://effective-altruism.com/ea/19d/update_on_effective_altruism_funds/ along with reasoning. Grantee approached Nick Beckstead with a grant proposal asking for 50000 USD. Beckstead provided all the money donated already from the far future fund, and made up the remainder via the EA Giving Group and some personal funds',NULL,NULL),
  # -- Q3 2018 update
  ('Effective Altruism Funds: Long-Term Future Fund','Centre for Effective Altruism',162537,'2018-08-14','day','donation log','Cause prioritization','https://app.effectivealtruism.org/funds/far-future/payouts/6g4f7iae5Ok6K6YOaAiyK0','https://app.effectivealtruism.org/funds/far-future','Nick Beckstead',917000,0.1772,'Grant made from the Long-Term Future Fund; an accompanying grant of $56,061 from the Effective Altruism Commmunity FUnd was also made. Beckstead recommended. CEA will use the LTF funding to support a new project whose objective is to expand global priorities research in academia, especially related to issues around longtermism',NULL,NULL),
  ('Effective Altruism Funds: Long-Term Future Fund','Center for Applied Rationality',174021,'2018-08-14','day','donation log','Rationality improvement','https://app.effectivealtruism.org/funds/far-future/payouts/6g4f7iae5Ok6K6YOaAiyK0','https://app.effectivealtruism.org/funds/far-future','Nick Beckstead',917000,0.1898,'Grant made from the Long-Term Future Fund. Beckstead recommended that the grantee spend the money to save time and increase productivity of employees (for instance, by subsidizing childcare or electronics)',NULL,NULL),
  ('Effective Altruism Funds: Long-Term Future Fund','Machine Intelligence Research Institute',488994,'2018-08-14','day','donation log','AI safety','https://app.effectivealtruism.org/funds/far-future/payouts/6g4f7iae5Ok6K6YOaAiyK0','https://app.effectivealtruism.org/funds/far-future','Nick Beckstead',917000,0.5332,'Grant made from the Long-Term Future Fund. Beckstead recommended that the grantee spend the money to save time and increase productivity of employees (for instance, by subsidizing childcare or electronics)',NULL,NULL),
  # -- Q4 2018 update
  ('Effective Altruism Funds: Long-Term Future Fund','Machine Intelligence Research Institute',40000,'2018-11-29','day','donation log','AI safety','https://app.effectivealtruism.org/funds/far-future/payouts/3JnNTzhJQsu4yQAYcKceSi','https://app.effectivealtruism.org/funds/far-future','Alex Zhu|Helen Toner|Matt Fallshaw|Matt Wage|Oliver Habryka',95500,0.4188,'Grant made from the Long-Term Future Fund. Donor believes that the new research directions outlined by donee at https://intelligence.org/2018/11/22/2018-update-our-new-research-directions/ are promising, and donee fundraising post suggests it could productively absorb additional funding',NULL,NULL),
  ('Effective Altruism Funds: Long-Term Future Fund','Ought',10000,'2018-11-29','day','donation log','AI safety','https://app.effectivealtruism.org/funds/far-future/payouts/3JnNTzhJQsu4yQAYcKceSi','https://app.effectivealtruism.org/funds/far-future','Alex Zhu|Helen Toner|Matt Fallshaw|Matt Wage|Oliver Habryka',95500,0.1047,'Grant made to implement AI alignment concepts in real-world applications. Donee seems more hiring-constrained than fundraising-constrained, hence only a small amount, but donor does believe that donee has a promising approach',NULL,NULL),
  ('Effective Altruism Funds: Long-Term Future Fund','AI summer school',21000,'2018-11-29','day','donation log','AI safety','https://app.effectivealtruism.org/funds/far-future/payouts/3JnNTzhJQsu4yQAYcKceSi','https://app.effectivealtruism.org/funds/far-future','Alex Zhu|Helen Toner|Matt Fallshaw|Matt Wage|Oliver Habryka',95500,0.2199,'Grant to fund the second year of a summer school on AI safety, aiming to familiarize potential researchers with interesting technical problems in the field. Last year’s iteration of this event appears to have gone well, per https://www.lesswrong.com/posts/bXLi3n2jrfqRwoSTH/human-aligned-ai-summer-school-a-summary and private information available to donor. Donor believes that well-run education efforts of this kind are valuable (where “well-run” refers to the quality of the intellectual content, the participants, and the logistics of the event), and feels confident enough that this particular effort will be well-run',NULL,NULL),
  ('Effective Altruism Funds: Long-Term Future Fund','Foretold',20000,'2018-11-29','day','donation log','Forecasting','https://app.effectivealtruism.org/funds/far-future/payouts/3JnNTzhJQsu4yQAYcKceSi','https://app.effectivealtruism.org/funds/far-future','Alex Zhu|Helen Toner|Matt Fallshaw|Matt Wage|Oliver Habryka',95500,0.2094,'Ozzie Gooen sought funding to build an online community of EA forecasters, researchers, and data scientists to predict variables of interest to the EA community. Ozzie proposed using the platform to answer a range of questions, including examples like “How many Google searches will there be for reinforcement learning in 2020?” or “How many plan changes will 80,000 hours cause in 2020?”, and using the results to help EA organizations and individuals to prioritize. The grant decision was made based on past success by Ozzie Gooen with Guesstimate https://www.getguesstimate.com/ as well as belief both in the broad value of the project and the specifics of the project plan. The project would later be named Foretold, and receive a followup grant of $70,000 in the April 2019 round of grants https://app.effectivealtruism.org/funds/far-future/payouts/6vDsjtUyDdvBa3sNeoNVvl from the Long Term Future Fund; see also https://forum.effectivealtruism.org/posts/CJJDwgyqT4gXktq6g/long-term-future-fund-april-2019-grant-decisions for more detail',NULL,'Ozzie Gooen'),
  ('Effective Altruism Funds: Long-Term Future Fund','AI Safety Unconference',4500,'2018-11-29','day','donation log','AI safety','https://app.effectivealtruism.org/funds/far-future/payouts/3JnNTzhJQsu4yQAYcKceSi','https://app.effectivealtruism.org/funds/far-future','Alex Zhu|Helen Toner|Matt Fallshaw|Matt Wage|Oliver Habryka',95500,0.0471,'Orpheus Lummis and Vaughn DiMarco are organizing an unconference on AI Alignment on the last day of the NeurIPS conference, with the goal of facilitating networking and research on AI Alignment among a diverse audience of AI researchers with and without safety backgrounds. Based on interaction with the organizers and some participants, the donor feels this project is worth funding. However, the donee is still not sure if the unconference will be held, so the grant is conditional to the donee deciding to proceed. The grant would fully fund the request',NULL,NULL);

/* Grants to 80,000 Hours */
insert into donations(donor, donee, amount, donation_date, donation_date_precision, donation_date_basis, cause_area, url, donor_cause_area_url, influencer, goal_amount, fraction, affected_countries, donation_earmark, donation_process, intended_use_of_funds_category, intended_use_of_funds, intended_funding_timeframe_in_months, donor_donee_reason, donor_amount_reason, donor_timing_reason, donor_next_donation_thoughts, donor_retrospective, notes) values
  ('Effective Altruism Funds: Long-Term Future Fund','80,000 Hours',91450,'2018-08-14','day','donation log','Effective altruism/movement growth/career counseling','https://app.effectivealtruism.org/funds/far-future/payouts/6g4f7iae5Ok6K6YOaAiyK0','https://app.effectivealtruism.org/funds/far-future','Nick Beckstead',917000,0.0997,NULL,NULL,
  /* donation_process */ 'The grant from the EA Long Term Future Fund is part of a final set of grant decisions being made by Nick Beckstead (granting $526,000 from the EA Meta Fund and $917,000 from the EA Long Term Future Fund) as he transitions out of managing both funds. Due to time constraints, Beckstead primarily relies on investigation of the organization done by the Open Philanthropy Project when making its 2017 grant https://www.openphilanthropy.org/giving/grants/80000-hours-general-support and 2018 renewal https://www.openphilanthropy.org/giving/grants/80000-hours-general-support-2018',
  /* intended_use_of_funds_category */ 'Organizational general support',
  /* intended_use_of_funds */ 'Beckstead writes "I recommended these grants with the suggestion that these grantees look for ways to use funding to trade money for saving the time or increasing the productivity of their employees (e.g. subsidizing electronics upgrades or childcare), due to a sense that (i) their work is otherwise much less funding constrained than it used to be, and (ii) spending like this would better reflect the value of staff time and increase staff satisfaction. However, I also told them that I was open to them using these funds to accomplish this objective indirectly (e.g. through salary increases) or using the funds for another purpose if that seemed better to them."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ 'The grant page references https://www.openphilanthropy.org/giving/grants/80000-hours-general-support-2018 for Beckstead''s opinion of the donee. This grant page is short, and in turn links to https://www.openphilanthropy.org/giving/grants/80000-hours-general-support which has a detailed Case for the grant section https://www.openphilanthropy.org/giving/grants/80000-hours-general-support#Case_for_the_grant that praises 80,000 Hours'' track record in terms of impact-adjusted significant plan changes (IASPCs)',
  /* donor_amount_reason */ 'Beckstead is also recommending funding from the EA Meta Fund of $75,818 for 80,000 Hours. The grant page says "The amounts I’m granting out to different organizations are roughly proportional to the number of staff they have, with some skew towards MIRI that reflects greater EA Funds donor interest in the Long-Term Future Fund." Also: "I think a number of these organizations could qualify for the criteria of either the Long-Term Future Fund or the EA Community Fund because of their dual focus on EA and longtermism, which is part of the reason that 80,000 Hours is receiving a grant from each fund."',
  /* donor_timing_reason */ 'Timing determined by the timing of this round of grants, which is in turn determined by the need for Beckstead to grant out the money before handing over management of the fund',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ 'Even after the fund management being moved to a new team, the EA Meta Fund would continue making grants to 80,000 Hours. In fact, 80,000 Hours would receive grant money in each of the three subsequent grant rounds. However, the EA Long Term Future Fund would make no further grants to 80,000 Hours. This suggests that the selection of the grantee as a Long Term Future Fund grantee would not continue to be endorsed by the new management team',
  /* notes */ NULL);

/* April 2019 grants */
insert into donations(donor, donee, amount, donation_date, donation_date_precision, donation_date_basis, cause_area, url, donor_cause_area_url, influencer, goal_amount, fraction, affected_countries, donation_earmark, donation_process, intended_use_of_funds_category, intended_use_of_funds, intended_funding_timeframe_in_months, donor_donee_reason, donor_amount_reason, donor_timing_reason, donor_next_donation_thoughts, donor_retrospective, notes) values
  ('Effective Altruism Funds: Long-Term Future Fund','Effective Altruism Zürich',17900,'2019-04-07','day','donation log','AI safety','https://app.effectivealtruism.org/funds/far-future/payouts/6vDsjtUyDdvBa3sNeoNVvl','https://app.effectivealtruism.org/funds/far-future','Helen Toner|Matt Wage|Matt Fallshaw|Alex Zhu|Oliver Habryka',923150,0.0193,NULL,'Alex Lintz',
  /* donation_process */ 'Donee submitted grant application through the application form for the April 2019 round of grants from the Long Term Future Fund, and was selected as a grant recipient (23 out of almost 100 applications were accepted)',
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'A two-day workshop by Alex Lintz and collaborators from EA Zürich for effective altruists interested in AI governance careers, with the goals of giving participants background on the space, offering career advice, and building community.',
  /* intended_funding_timeframe_in_months */ 1,
  /* donor_donee_reason */ 'Donor writes: "We agree with their assessment that this space is immature and hard to enter, and believe their suggested plan for the workshop looks like a promising way to help participants orient to careers in AI governance."',
  /* donor_amount_reason */ 'Likely to be the amount requested by the donee in the application (this is not stated explicitly by either the donor or the donee)',
  /* donor_timing_reason */ 'Timing determined by timing of grant round. No specific timing-related considerations are discussed',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ 'The grant reasoning is written up by Helen Toner and is also included in the cross-post of the grant decision to the Effective Altruism Forum at https://forum.effectivealtruism.org/posts/CJJDwgyqT4gXktq6g/long-term-future-fund-april-2019-grant-decisions but the comments on the post do not discuss this specific grant'),
  ('Effective Altruism Funds: Long-Term Future Fund','Tessa Alexanian',26250,'2019-04-07','day','donation log','Biosecurity and pandemic preparedness','https://app.effectivealtruism.org/funds/far-future/payouts/6vDsjtUyDdvBa3sNeoNVvl','https://app.effectivealtruism.org/funds/far-future','Matt Wage|Helen Toner|Matt Fallshaw|Alex Zhu|Oliver Habryka',923150,0.0284,NULL,NULL,
  /* donation_process */ 'Donee submitted grant application through the application form for the April 2019 round of grants from the Long Term Future Fund, and was selected as a grant recipient (23 out of almost 100 applications were accepted)',
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'A one day biosecurity summit, immediately following the SynBioBeta industry conference.',
  /* intended_funding_timeframe_in_months */ 1,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ 'Likely to be the amount requested by the donee in the application (this is not stated explicitly by either the donor or the donee)',
  /* donor_timing_reason */ 'Timing determined by timing of grant round. No specific timing-related considerations are discussed',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ 'The grant reasoning is written up by Matt Wage and is also included in the cross-post of the grant decision to the Effective Altruism Forum at https://forum.effectivealtruism.org/posts/CJJDwgyqT4gXktq6g/long-term-future-fund-april-2019-grant-decisions but the comments on the post do not discuss this specific grant'),
  ('Effective Altruism Funds: Long-Term Future Fund','Shahar Avin',40000,'2019-04-07','day','donation log','AI safety','https://app.effectivealtruism.org/funds/far-future/payouts/6vDsjtUyDdvBa3sNeoNVvl','https://app.effectivealtruism.org/funds/far-future','Matt Wage|Helen Toner|Matt Fallshaw|Alex Zhu|Oliver Habryka',923150,0.0433,NULL,NULL,
  /* donation_process */ 'Donee submitted grant application through the application form for the April 2019 round of grants from the Long Term Future Fund, and was selected as a grant recipient (23 out of almost 100 applications were accepted)',
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Hiring an academic research assistant and other miscellaneous research expenses, for scaling up scenario role-play for AI strategy research and training.',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ 'Donor writes: "We think positively of Shahar’s past work (for example this report), and multiple people we trust recommended that we fund him." The linked report is https://maliciousaireport.com/',
  /* donor_amount_reason */ 'Likely to be the amount requested by the donee in the application (this is not stated explicitly by either the donor or the donee)',
  /* donor_timing_reason */ 'Timing determined by timing of grant round. No specific timing-related considerations are discussed',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ 'The grant reasoning is written up by Matt Wage and is also included in the cross-post of the grant decision to the Effective Altruism Forum at https://forum.effectivealtruism.org/posts/CJJDwgyqT4gXktq6g/long-term-future-fund-april-2019-grant-decisions but the comments on the post do not discuss this specific grant'),
  ('Effective Altruism Funds: Long-Term Future Fund','Lucius Caviola',50000,'2019-04-07','day','donation log','Effective altruism/long-termism','https://app.effectivealtruism.org/funds/far-future/payouts/6vDsjtUyDdvBa3sNeoNVvl','https://app.effectivealtruism.org/funds/far-future','Matt Wage|Helen Toner|Matt Fallshaw|Alex Zhu|Oliver Habryka',923150,0.0542,NULL,NULL,
  /* donation_process */ 'Donee submitted grant application through the application form for the April 2019 round of grants from the Long Term Future Fund, and was selected as a grant recipient (23 out of almost 100 applications were accepted). Donee also applied to the EA Meta Fund (another of the Effective Altruism Funds) and the total funding for the donee was split between the funds',
  /* intended_use_of_funds_category */ 'Living expenses during research project',
  /* intended_use_of_funds */ 'Part of the costs for a 2-year postdoc at Harvard working with Professor Joshua Greene. Grantee plans to study the psychology of effective altruism and long-termism. The funding from the Long Term Future Fund is roughly intended to cover the part of the costs that corresponds to the work on long-termism',
  /* intended_funding_timeframe_in_months */ 24,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ 'Total funding requested by the donee appears to be $130,000. Of this, $80,000 is provided by the EA Meta Fund in their March 2019 grant round https://app.effectivealtruism.org/funds/ea-community/payouts/1hVfcvrzRbpXUWYht4bu3b to cover the donee''s work on effective altruism, while the remaining $50,000 is provided through this grant by the Long Term Future Fund, and covers the work on long-termism. The reason for splitting funding in this way is not articulated',
  /* donor_timing_reason */ 'Timing determined by timing of grant round. No specific timing-related considerations are discussed. However, the write-up for the $80,000 grant provided by the EA Meta Fund https://app.effectivealtruism.org/funds/ea-community/payouts/1hVfcvrzRbpXUWYht4bu3b calls the grant a "time-bounded, specific opportunity that requires funding to initiate and explore" and similar reasoning may also apply to the $50,000 Long Term Future Fund grant',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ 'The grant reasoning is written up by Matt Wage and is also included in the cross-post of the grant decision to the Effective Altruism Forum at https://forum.effectivealtruism.org/posts/CJJDwgyqT4gXktq6g/long-term-future-fund-april-2019-grant-decisions but the comments on the post do not discuss this specific grant'),
  ('Effective Altruism Funds: Long-Term Future Fund','Ought',50000,'2019-04-07','day','donation log','AI safety','https://app.effectivealtruism.org/funds/far-future/payouts/6vDsjtUyDdvBa3sNeoNVvl','https://app.effectivealtruism.org/funds/far-future','Matt Wage|Helen Toner|Matt Fallshaw|Alex Zhu|Oliver Habryka',923150,0.0542,NULL,NULL,
  /* donation_process */ 'Donee submitted grant application through the application form for the April 2019 round of grants from the Long Term Future Fund, and was selected as a grant recipient (23 out of almost 100 applications were accepted)',
  /* intended_use_of_funds_category */ 'Organization financial buffer',
  /* intended_use_of_funds */ 'No specific information is shared on how the funds will be used at the margin, but the general description gives an idea: "Ought is a nonprofit aiming to implement AI alignment concepts in real-world applications"',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ 'Donor is explicitly interesting in diversifying funder base for donee, who currently receives almost all its funding from only two sources and is trying to change that. Othewise, same reason as with last round of funds https://app.effectivealtruism.org/funds/far-future/payouts/3JnNTzhJQsu4yQAYcKceSi namely "We believe that Ought’s approach is interesting and worth trying, and that they have a strong team. [...] Part of the aim of the grant is to show Ought as an example of the type of organization we are likely to fund in the future."',
  /* donor_amount_reason */ 'In write-up for previous grant at https://app.effectivealtruism.org/funds/far-future/payouts/3JnNTzhJQsu4yQAYcKceSi of $10,000, donor says: "Our understanding is that hiring is currently more of a bottleneck for them than funding, so we are only making a small grant." The amount this time is bigger ($50,000) but the general principle likely continues to apply',
  /* donor_timing_reason */ 'In the previous grant round, donor had said "Part of the aim of the grant is to show Ought as an example of the type of organization we are likely to fund in the future." Thus, it makes sense to donate again in this round',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ 'The grant reasoning is written up by Matt Wage and is also included in the cross-post of the grant decision to the Effective Altruism Forum at https://forum.effectivealtruism.org/posts/CJJDwgyqT4gXktq6g/long-term-future-fund-april-2019-grant-decisions but the comments on the post do not discuss this specific grant'),
  ('Effective Altruism Funds: Long-Term Future Fund','Nikhil Kunapuli',30000,'2019-04-07','day','donation log','AI safety/deconfusion research','https://app.effectivealtruism.org/funds/far-future/payouts/6vDsjtUyDdvBa3sNeoNVvl','https://app.effectivealtruism.org/funds/far-future','Alex Zhu|Matt Wage|Helen Toner|Matt Fallshaw|Oliver Habryka',923150,0.0325,NULL,NULL,
  /* donation_process */ 'Donee submitted grant application through the application form for the April 2019 round of grants from the Long Term Future Fund, and was selected as a grant recipient (23 out of almost 100 applications were accepted)',
  /* intended_use_of_funds_category */ 'Living expenses during research project',
  /* intended_use_of_funds */ 'Grantee is doing independent deconfusion research for AI safety. His approach is to develop better foundational understandings of various concepts in AI safety, like safe exploration and robustness to distributional shift, by exploring these concepts in complex systems science and theoretical biology, domains outside of machine learning for which these concepts are also applicable.',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ 'Fund manager Alex Zhu says: "I recommended that we fund Nikhil because I think Nikhil’s research directions are promising, and because I personally learn a lot about AI safety every time I talk with him."',
  /* donor_amount_reason */ 'Likely to be the amount requested by the donee in the application (this is not stated explicitly by either the donor or the donee)',
  /* donor_timing_reason */ 'Timing determined by timing of grant round. No specific timing-related considerations are discussed',
  /* donor_next_donation_thoughts */ 'Alex Zhu, in his grant write-up, says that the quality of the work will be assessed by researchers at MIRI. Although it is not explicitly stated, it is likely that this evaluation will influence the decision of whether to make further grants',
  /* donor_retrospective */ NULL,
  /* notes */ 'The grant reasoning is written up by Alex Zhu and is also included in the cross-post of the grant decision to the Effective Altruism Forum at https://forum.effectivealtruism.org/posts/CJJDwgyqT4gXktq6g/long-term-future-fund-april-2019-grant-decisions but the comments on the post do not discuss this specific grant'),
  ('Effective Altruism Funds: Long-Term Future Fund','Anand Srinivasan',30000,'2019-04-07','day','donation log','AI safety/deconfusion research','https://app.effectivealtruism.org/funds/far-future/payouts/6vDsjtUyDdvBa3sNeoNVvl','https://app.effectivealtruism.org/funds/far-future','Alex Zhu|Matt Wage|Helen Toner|Matt Fallshaw|Oliver Habryka',923150,0.0325,NULL,NULL,
  /* donation process */ 'Donee submitted grant application through the application form for the April 2019 round of grants from the Long Term Future Fund, and was selected as a grant recipient (23 out of almost 100 applications were accepted)',
  /* intended_use_of_funds_category */ 'Living expenses during research project',
  /* intended_use_of_funds */ 'Grantee is doing independent deconfusion research for AI safety. His angle of attack is to develop a framework that will allow researchers to make provable claims about what specific AI systems can and cannot do, based off of factors like their architectures and their training processes.',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ 'Grantee worked with main grant influencer Alex Zhu at an enterprise software company that they cofounded. Alex Zhu says in his grant write-up: "I recommended that we fund Anand because I think Anand’s research directions are promising, and I personally learn a lot about AI safety every time I talk with him."',
  /* donor_amount_reason */ 'Likely to be the amount requested by the donee in the application (this is not stated explicitly by either the donor or the donee)',
  /* donor_timing_reason */ 'Timing determined by timing of grant round. No specific timing-related considerations are discussed',
  /* donor_next_donation_thoughts */ 'Alex Zhu, in his grant write-up, says that the quality of the work will be assessed by researchers at MIRI. Although it is not explicitly stated, it is likely that this evaluation will influence the decision of whether to make further grants',
  /* donor_retrospective */ NULL,
  /* notes */ 'The quality of grantee''s work will be judged by researchers at the Machine Intelligence Research Institute. The grant reasoning is written up by Alex Zhu and is also included in the cross-post of the grant decision to the Effective Altruism Forum at https://forum.effectivealtruism.org/posts/CJJDwgyqT4gXktq6g/long-term-future-fund-april-2019-grant-decisions but the comments on the post do not discuss this specific grant'),
  ('Effective Altruism Funds: Long-Term Future Fund','Effective Altruism Russia',28000,'2019-04-07','day','donation log','Rationality improvement','https://app.effectivealtruism.org/funds/far-future/payouts/6vDsjtUyDdvBa3sNeoNVvl','https://app.effectivealtruism.org/funds/far-future','Oliver Habryka|Alex Zhu|Matt Wage|Helen Toner|Matt Fallshaw',923150,0.0303,NULL,'Mikhail Yugadin',
  /* donation_process */ 'Donee submitted grant application through the application form for the April 2019 round of grants from the Long Term Future Fund, and was selected as a grant recipient (23 out of almost 100 applications were accepted)',
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant to Mikhail Yugadin for Effective Altruism Russia to give copies of Harry Potter and the Methods of Rationality to the winners of EGMO 2019 and IMO 2020.',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ 'In the grant write-up, Oliver Habryka explains his evaluation of the grant as based on three questions: (1) What effects does reading HPMOR have on people? (2) How good of a target group are Math Olympiad winners for these effects? (3) Is the team competent enough to execute on their plan?',
  /* donor_amount_reason */ 'Likely to be the amount requested by the donee in the application (this is not stated explicitly by either the donor or the donee). The comments include more discussion of the unit economics of the grant, and whether the effective cost of $43/copy is reasonable',
  /* donor_timing_reason */ 'Timing determined by timing of grant round. No specific timing-related considerations are discussed. The need to secure money in advance of the events for which the money will be used likely affected the timing of the application',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ 'The grant reasoning is written up by Oliver Habryka and is available at https://forum.effectivealtruism.org/posts/CJJDwgyqT4gXktq6g/long-term-future-fund-april-2019-grant-decisions  There is a lot of criticism and discussion of the grant in the comments'),
  ('Effective Altruism Funds: Long-Term Future Fund','Alex Turner',30000,'2019-04-07','day','donation log','AI safety/agent foundations','https://app.effectivealtruism.org/funds/far-future/payouts/6vDsjtUyDdvBa3sNeoNVvl','https://app.effectivealtruism.org/funds/far-future','Oliver Habryka|Alex Zhu|Matt Wage|Helen Toner|Matt Fallshaw',923150,0.0325,NULL,NULL,
  /* donation_process */ 'Donee submitted grant application through the application form for the April 2019 round of grants from the Long Term Future Fund, and was selected as a grant recipient (23 out of almost 100 applications were accepted)',
  /* intended_use_of_funds_category */ 'Living expenses during research project',
  /* intended_use_of_funds */ 'Grant for building towards a “Limited Agent Foundations” thesis on mild optimization and corrigibility. Grantee is a third-year computer science PhD student funded by a graduate teaching assistantship; to dedicate more attention to alignment research, he is applying for one or more trimesters of funding (spring term starts April 1).',
  /* intended_funding_timeframe_in_months */ 4,
  /* donor_donee_reason */ 'In the grant write-up, Oliver Habryka explains that he is excited by (a) Turner''s posts to LessWrong reviewing many math textbooks useful for thinking about the alignment problem, (b) Turner not being intimidated by the complexity of the problem, and (c) Turner writing up his thoughts and hypotheses in a clear way, seeking feedback on them early, and making a set of novel contributions to an interesting sub-field of AI Alignment quite quickly (in the form of his work on impact measures, on which he recently collaborated with the DeepMind AI Safety team).',
  /* donor_amount_reason */ 'Likely to be the amount requested by the donee in the application (this is not stated explicitly by either the donor or the donee)',
  /* donor_timing_reason */ 'Timing determined by timing of grant round. No specific timing-related considerations are discussed',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ 'The grant reasoning is written up by Oliver Habryka and is available at https://forum.effectivealtruism.org/posts/CJJDwgyqT4gXktq6g/long-term-future-fund-april-2019-grant-decisions  The comments on the post do not discuss this specific grant'),
  ('Effective Altruism Funds: Long-Term Future Fund','David Girardo',30000,'2019-04-07','day','donation log','AI safety/deconfusion research','https://app.effectivealtruism.org/funds/far-future/payouts/6vDsjtUyDdvBa3sNeoNVvl','https://app.effectivealtruism.org/funds/far-future','Alex Zhu|Matt Wage|Helen Toner|Matt Fallshaw|Oliver Habryka',923150,0.0325,NULL,NULL,
  /* donation_process */ 'Donee submitted grant application through the application form for the April 2019 round of grants from the Long Term Future Fund, and was selected as a grant recipient (23 out of almost 100 applications were accepted)',
  /* intended_use_of_funds_category */ 'Living expenses during research project',
  /* intended_use_of_funds */ 'Grantee is doing independent deconfusion research for AI safety. His angle of attack is to elucidate the ontological primitives for representing hierarchical abstractions, drawing from his experience with type theory, category theory, differential geometry, and theoretical neuroscience.',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ 'The main investigator and influencer for the grant, Alex Zhu, finds the research directions promising. Tsvi Benson-Tilsen, a MIRI researcher, has also recommended that grantee get funding.',
  /* donor_amount_reason */ 'Likely to be the amount requested by the donee in the application (this is not stated explicitly by either the donor or the donee)',
  /* donor_timing_reason */ 'Timing determined by timing of grant round. No specific timing-related considerations are discussed',
  /* donor_next_donation_thoughts */ 'The quality of the grantee''s work will be assessed by researchers at MIRI',
  /* donor_retrospective */ NULL,
  /* notes */ ' The grant reasoning is written up by Alex Zhu and is also included in the cross-post of the grant decision to the Effective Altruism Forum at https://forum.effectivealtruism.org/posts/CJJDwgyqT4gXktq6g/long-term-future-fund-april-2019-grant-decisions but the comments on the post do not discuss this specific grant'),
  ('Effective Altruism Funds: Long-Term Future Fund','Tegan McCaslin',30000,'2019-04-07','day','donation log','AI safety/forecasting','https://app.effectivealtruism.org/funds/far-future/payouts/6vDsjtUyDdvBa3sNeoNVvl','https://app.effectivealtruism.org/funds/far-future','Oliver Habryka|Alex Zhu|Matt Wage|Helen Toner|Matt Fallshaw',923150,0.0325,NULL,NULL,
  /* donation_process */ 'Donee submitted grant application through the application form for the April 2019 round of grants from the Long Term Future Fund, and was selected as a grant recipient (23 out of almost 100 applications were accepted)',
  /* intended_use_of_funds_category */ 'Living expenses during research project',
  /* intended_use_of_funds */ 'Grant for independent research projects relevant to AI forecasting and strategy, including (but not necessarily limited to) some of the following: (1) Does the trajectory of AI capability development match that of biological evolution? (2) How tractable is long-term forecasting? (3) How much compute did evolution use to produce intelligence? (4)Benchmarking AI capabilities against insects. Short doc on (1) and (2) at https://docs.google.com/document/d/1hTLrLXewF-_iJiefyZPF6L677bLrUTo2ziy6BQbxqjs/edit',
  /* intended_funding_timeframe_in_months */ 6,
  /* donor_donee_reason */ 'Reasons for the grant from Oliver Habryka, the main influencer, include: (1) It''s easier to relocate someone who has already demonstrated trust and skills than to find someone completely new, (2.1) It''s important to give good researchers runway while they find the right place. Habryka notes: "my brief assessment of Tegan’s work was not the reason why I recommended this grant, and if Tegan asks for a new grant in 6 months to focus on solo research, I will want to spend significantly more time reading her output and talking with her, to understand how these questions were chosen and what precise relation they have to forecasting technological progress in AI."',
  /* donor_amount_reason */ 'Likely to be the amount requested by the donee in the application (this is not stated explicitly by either the donor or the donee). Habryka also mentions that he is interested only in providing limited runway, and would need to assess much more carefully for a more long-term grant',
    /* donor_timing_reason */ 'Timing determined by timing of grant round. However, it is also related to the grantee''s situation (she has just quit her job at AI Impacts, and needs financial runway to continue pursuing promising research projects)',
  /* donor_next_donation_thoughts */ 'The grant investigator Oliver Habryka notes: "if Tegan asks for a new grant in 6 months to focus on solo research, I will want to spend significantly more time reading her output and talking with her, to understand how these questions were chosen and what precise relation they have to forecasting technological progress in AI."',
  /* donor_retrospective */ NULL,
  /* notes */ 'The grant reasoning is written up by Oliver Habryka and is available at https://forum.effectivealtruism.org/posts/CJJDwgyqT4gXktq6g/long-term-future-fund-april-2019-grant-decisions  The comments on the post do not discuss this specific grant, but a grant to Lauren Lee that includes somewhat similar reasoning (providing people runway after they leave their jobs, so they can explore better) attracts some criticism'),
  ('Effective Altruism Funds: Long-Term Future Fund','Metaculus',70000,'2019-04-07','day','donation log','Forecasting','https://app.effectivealtruism.org/funds/far-future/payouts/6vDsjtUyDdvBa3sNeoNVvl','https://app.effectivealtruism.org/funds/far-future','Oliver Habryka|Alex Zhu|Matt Wage|Helen Toner|Matt Fallshaw',923150,0.0758,NULL,'Anthony Aguirre',
  /* donation_process */ 'Donee submitted grant application through the application form for the April 2019 round of grants from the Long Term Future Fund, and was selected as a grant recipient (23 out of almost 100 applications were accepted)',
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant to Anothony Aguirre to expand the Metaculus prediction platform along with its community. Metaculus.com is a fully-functional prediction platform with ~10,000 registered users and >120,000 predictions made to date on more than >1000 questions. The two major high-priority expansions are: (1) An integrated set of extensions to improve user interaction and information-sharing. This would include private messaging and notifications, private groups, a prediction “following” system to create micro-teams within individual questions, and various incentives and systems for information-sharing. (2) Link questions into a network. Users would express links between questions, from very simple (“notify me regarding question Y when P(X) changes substantially) to more complex (“Y happens only if X happens, but not conversely”, etc.) Information can also be gleaned from what users actually do.',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ 'The grant investigator and main influencer, Oliver Habryka, refers to reasoning included in the grant to Ozzie Gooen for Foretold, that is made in the same batch of grants and described at https://forum.effectivealtruism.org/posts/CJJDwgyqT4gXktq6g/long-term-future-fund-april-2019-grant-decisions He also lists these reasons for liking Metaculus: (1) Valuable service in the past few years, (2) Cooperation with the X-risk space to get answers to important questions',
  /* donor_amount_reason */ 'The grantee requested $150,000, but Oliver Habryka, the grant investigator, was not confident enough in the grant to recommend the full amount. Some concerns mentioned: (1) Lack of a dedicated full-time resource, (2) Overlap with the Good Judgment Project, that reduces its access to resources and people',
  /* donor_timing_reason */ 'Timing determined by timing of grant round',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ 'The grant reasoning is written up by Oliver Habryka and is available at https://forum.effectivealtruism.org/posts/CJJDwgyqT4gXktq6g/long-term-future-fund-april-2019-grant-decisions The comments discuss this and the other forecasting grants, and include the question "why are you acting as grant-givers here rather than as special interest investors?" It is also included in a list of potentially concerning grants in a portfolio evaluation comment https://forum.effectivealtruism.org/posts/CJJDwgyqT4gXktq6g/long-term-future-fund-april-2019-grant-decisions#d4YHzSJnNWmyxf6HM by Evan Gaensbauer'),
  ('Effective Altruism Funds: Long-Term Future Fund','Foretold',70000,'2019-04-07','day','donation log','Forecasting','https://app.effectivealtruism.org/funds/far-future/payouts/6vDsjtUyDdvBa3sNeoNVvl','https://app.effectivealtruism.org/funds/far-future','Oliver Habryka|Alex Zhu|Matt Wage|Helen Toner|Matt Fallshaw',923150,0.0758,NULL,'Ozzie Gooen',
  /* donation_process */ 'Donee submitted grant application through the application form for the April 2019 round of grants from the Long Term Future Fund, and was selected as a grant recipient (23 out of almost 100 applications were accepted)',
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant will be mainly used by Ozzie Gooen to pay programmers to work on Foretold at http://www.foretold.io/ a forecasting application that handles full probability distributions. This includes work on Ken.js, a private version of Wikidata that Gooen has started integrating with Foretold',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ 'Grant investigator and main influencer Oliver Habryka gives these reasons for the grant, as well as other forecasting-related grants made to Anthony Aguirre (Metaculus) and Jacob Lagerros: (1) confusion about what is progress and what problems need solving, (2) need for many people to collaborate and document, (3) low-hanging fruit in designing better online platforms for making intellectual progress -- Habryka works on LessWrong 2.0 for that reason, and Gooen has past experience in the space with his building of Guesstimate, (4) promise and tractability for forecasting platforms in particular (for instance, work by Philip Tetlock and work by Robin Hanson), (5) Even though some platforms, such as Predictionbook and Guesstimate, did not get the traction they expected, others like the Good Judgment Project have been successful, so one should not overgeneralize from a few failures. In addition, Habryka has a positive impression of Gooen in both in-person interaction and online writing',
  /* donor_amount_reason */ 'Likely to be the amount requested by the donee in the application (this is not stated explicitly by either the donor or the donee)',
  /* donor_timing_reason */ 'Timing determined partly by timing of grant round. Gooen was a recipient of a previous $20,000 grant from the same fund (the EA Long Term Future Fund) and found the money very helpful. He applied for more money in this round to scale the project up further',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ 'The grant reasoning is written up by Oliver Habryka and is available at https://forum.effectivealtruism.org/posts/CJJDwgyqT4gXktq6g/long-term-future-fund-april-2019-grant-decisions The comments discuss this and the other forecasting grants, and include the question "why are you acting as grant-givers here rather than as special interest investors?" It is also included in a list of potentially concerning grants in a portfolio evaluation comment https://forum.effectivealtruism.org/posts/CJJDwgyqT4gXktq6g/long-term-future-fund-april-2019-grant-decisions#d4YHzSJnNWmyxf6HM by Evan Gaensbauer'),
  ('Effective Altruism Funds: Long-Term Future Fund','Machine Intelligence Research Institute',50000,'2019-04-07','day','donation log','AI safety','https://app.effectivealtruism.org/funds/far-future/payouts/6vDsjtUyDdvBa3sNeoNVvl','https://app.effectivealtruism.org/funds/far-future','Oliver Habryka|Alex Zhu|Matt Wage|Helen Toner|Matt Fallshaw',923150,0.0542,NULL,NULL,
  /* donation_process */ 'Donee submitted grant application through the application form for the April 2019 round of grants from the Long Term Future Fund, and was selected as a grant recipient (23 out of almost 100 applications were accepted)',
  /* intended_use_of_funds_category */ 'Organizational general support',
  /* intended_use_of_funds */ NULL,
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ 'Grant investigation and influencer Oliver Habryka believes that MIRI is making real progress in its approach of "creating a fundamental piece of theory that helps humanity to understand a wide range of powerful phenomena" He notes that MIRI started work on the alignment problem long before it became cool, which gives him more confidence that they will do the right thing and even their seemingly weird actions may be justified in ways that are not yet obvious. He also thinks that both the research team and ops staff are quite competent',
  /* donor_amount_reason */ 'Habryka offers the following reasons for giving a grant of just $50,000, which is small relative to the grantee budget: (1) MIRI is in a solid position funding-wise, and marginal use of money may be lower-impact. (2) There is a case for investing in helping grow a larger and more diverse set of organizations, as opposed to putting money in a few stable and well-funded onrganizations.',
  /* donor_timing_reason */ 'Timing determined by timing of grant round',
  /* donor_next_donation_thoughts */ 'Oliver Habryka writes: "I can see arguments that we should expect additional funding for the best teams to be spent well, even accounting for diminishing margins, but on the other hand I can see many meta-level concerns that weigh against extra funding in such cases. Overall, I find myself confused about the marginal value of giving MIRI more money, and will think more about that between now and the next grant round."',
  /* donor_retrospective */ NULL,
  /* notes */ 'The grant reasoning is written up by Oliver Habryka and is available at https://forum.effectivealtruism.org/posts/CJJDwgyqT4gXktq6g/long-term-future-fund-april-2019-grant-decisions . Despite these, Habryka recommends a relatively small grant to MIRI, because they are already relatively well-funded and are not heavily bottlenecked on funding. However, he ultimately decides to grant some amount to MIRI, giving some explanation. He says he will think more about this before the next funding round'),
  ('Effective Altruism Funds: Long-Term Future Fund','Center for Applied Rationality',150000,'2019-04-07','day','donation log','Rationality improvement','https://app.effectivealtruism.org/funds/far-future/payouts/6vDsjtUyDdvBa3sNeoNVvl','https://app.effectivealtruism.org/funds/far-future','Oliver Habryka|Alex Zhu|Matt Wage|Helen Toner|Matt Fallshaw',923150,0.1625,NULL,NULL,
  /* donation_process */ 'Donee submitted grant application through the application form for the April 2019 round of grants from the Long Term Future Fund, and was selected as a grant recipient (23 out of almost 100 applications were accepted)',
  /* intended_use_of_funds_category */ 'Organizational general support',
  /* intended_use_of_funds */ 'The grant is to help the Center for Applied Rationality (CFAR) survive as an organization for the next few months (i.e., till the next grant round, which is 3 months later) without having to scale down operations. CFAR is low on finances because they did not run a 2018 fundraiser. because they felt that running a fundraiser would be in bad taste after what they considered a messup on their part in the Brent Dill situation',
  /* intended_funding_timeframe_in_months */ 3,
  /* donor_donee_reason */ 'Grant investigator and main influencer Oliver Habryka thinks CFAR intro workshops have had positive impact in 3 ways: (1) establishing epistemic norms, (2) training, and (3) recruitment into the X-risk network (especially AI safety). He also thinks CFAR faces many challenges, including the departure of many key employees, the difficulty of attracting top talent, and a dilution of its truth-seeking focus. However, he is enthusiastic about joint CFAR/MIRI workshops for programmers, where CFAR provides instructors. His final reason for donating is to avoid CFAR having to scale down due to its funding shortfall because it didn''t run the 2018 fundraiser',
  /* donor_amount_reason */ 'The grant amount, which is the largest in this grant round from the EA Long Term Future Fund, is chosen to be sufficient for CFAR to continue operating as usual till the next grant round from the EA Long Term Future Fund (in about 3 months). Habryka further elaborates in https://forum.effectivealtruism.org/posts/CJJDwgyqT4gXktq6g/long-term-future-fund-april-2019-grant-recommendations#uhH4ioNbdaFrwGt4e in reply to Milan Griffes, explaining why the grant is large and unrestricted',
  /* donor_timing_reason */ 'Timing determined by timing of grant round, as well as by CFAR''s time-sensitive financial situation; the grant round is a few months after the end of 2018, so the shortfall of funds raised because of not conducting the 2018 fundraiser is starting to hit on the finances',
  /* donor_next_donation_thoughts */ 'Grant investigator and main influencer Oliver Habryka writes: "I didn’t have enough time this grant round to understand how the future of CFAR will play out; the current grant amount seems sufficient to ensure that CFAR does not have to take any drastic action until our next grant round. By the next grant round, I plan to have spent more time learning and thinking about CFAR’s trajectory and future, and to have a more confident opinion about what the correct funding level for CFAR is."',
  /* donor_retrospective */ NULL,
  /* notes */ 'The grant reasoning is written up by Oliver Habryka and is available at https://forum.effectivealtruism.org/posts/CJJDwgyqT4gXktq6g/long-term-future-fund-april-2019-grant-decisions In the comments, Milan Griffes asks why such a large, unrestricted grant is being made to CFAR despite these concerns, and also what Habryka hopes to learn about CFAR before the next grant round. There are replies from Peter McCluskey and Habryka, with some further comment back-and-forth'),
  ('Effective Altruism Funds: Long-Term Future Fund','AI Safety Camp',25000,'2019-04-07','day','donation log','AI safety','https://app.effectivealtruism.org/funds/far-future/payouts/6vDsjtUyDdvBa3sNeoNVvl','https://app.effectivealtruism.org/funds/far-future','Oliver Habryka|Alex Zhu|Matt Wage|Helen Toner|Matt Fallshaw',923150,0.0271,NULL,'Johannes Heidecke',
  /* donation_process */ 'Donee submitted grant application through the application form for the April 2019 round of grants from the Long Term Future Fund, and was selected as a grant recipient (23 out of almost 100 applications were accepted)',
  /* intended_use_of_funds_category */ 'Organizational general support',
  /* intended_use_of_funds */ 'Grant to fund an upcoming camp in Madrid being organized by AI Safety Camp in April 2019. The camp consists of several weeks of online collaboration on concrete research questions, culminating in a 9-day intensive in-person research camp. The goal is to support aspiring researchers of AI alignment to boost themselves into productivity.',
  /* intended_funding_timeframe_in_months */ 1,
  /* donor_donee_reason */ 'The grant investigator and main influencer Oliver Habryka mentions that: (1) He has a positive impression of the organizers and has received positive feedback from participants in the first two AI Safety Camps. (2) A greater need to improve access to opportunities in AI alignment for people in Europe. Habryka also mentions an associated greater risk of making the AI Safety Camp the focal point of the AI safety community in Europe, which could cause problems if the quality of the people involved isn''t high. He mentions two more specific concerns: (a) Organizing long in-person events is hard, and can lead to conflict, as the last two camps did. (b) People who don''t get along with the organizers may find themselves shut out of the AI safety network',
  /* donor_amount_reason */  'Likely to be the amount requested by the donee in the application (this is not stated explicitly by either the donor or the donee)',
  /* donor_timing_reason */ 'Timing determined by the timing of the camp (which is scheduled for April 2019; the grant is being made around the same time) as well as the timing of the grant round',
  /* donor_next_donation_thoughts */ 'Grant investigator and main influencer Habryka writes: "I would want to engage with the organizers a fair bit more before recommending a renewal of this grant"',
  /* donor_retrospective */ NULL,
  /* notes */ 'Grantee in the grant document is listed as Johannes Heidecke, but the grant is for the AI Safety Camp. Grant is for supporting  The grant reasoning is written up by Oliver Habryka and is available at https://forum.effectivealtruism.org/posts/CJJDwgyqT4gXktq6g/long-term-future-fund-april-2019-grant-decisions Grant decision was coordinated with Effective Altruism Grants (specifically, Nicole Ross of CEA) who had considered also making a grant to the camp. Effective Altruism Grants ultimately decided against making the grant, and the Long Term Future Fund made it instead. Nicole Ross, in the evaluation by EA Grants, mentions the same concerns that Habryka does: interpersonal conflict and people being shut out of the AI safety community if they don''t get along with the camp organizers'),
  ('Effective Altruism Funds: Long-Term Future Fund','Robert Miles',39000,'2019-04-07','day','donation log','AI safety/content creation/video','https://app.effectivealtruism.org/funds/far-future/payouts/6vDsjtUyDdvBa3sNeoNVvl','https://app.effectivealtruism.org/funds/far-future','Oliver Habryka|Alex Zhu|Matt Wage|Helen Toner|Matt Fallshaw',923150,0.0422,NULL,NULL,
  /* donation_process */ 'Donee submitted grant application through the application form for the April 2019 round of grants from the Long Term Future Fund, and was selected as a grant recipient (23 out of almost 100 applications were accepted)',
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant to create video content on AI alignment. Grantee has a YouTube channel at https://www.youtube.com/channel/UCLB7AzTwc6VFZrBsO2ucBMg (average 20,000 views per video) and also creates videos for the Computerphile channel https://www.youtube.com/watch?v=3TYT1QfdfsM&t=2s (often more than 100,000 views per video)',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ 'Grant investigator and main influencer Oliver Habryka favors the grant for these reasons: (1) Grantee explains AI alignment as primarily a technical problem, not a moral or political problem, (2) Grantee does not politicize AI safety, (3) Grantee''s goal is to create interest in these problems from future researchers, and not to simply get as large of an audience as possible. Habryka notes that the grantee is the first skilled person in the X-risk community working full-time on producing video content. "Being the very best we have in this skill area, he is able to help the community in a number of novel ways (for example, he’s already helping existing organizations produce videos about their ideas)." In the previous grant round, the grantee had requested funding for a collaboration with RAISE to produce videos for them, but Habryka felt it was better to fund the grantee directly and allow him to decide which organizations he wanted to help with his videos',
  /* donor_amount_reason */ 'Likely to be the amount requested by the donee in the application (this is not stated explicitly by either the donor or the donee)',
  /* donor_timing_reason */ 'Timing determined by timing of grant round',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ 'The grant reasoning is written up by Oliver Habryka and is available at https://forum.effectivealtruism.org/posts/CJJDwgyqT4gXktq6g/long-term-future-fund-april-2019-grant-decisions'),
  ('Effective Altruism Funds: Long-Term Future Fund','Jacob Lagerros',27000,'2019-04-07','day','donation log','AI safety/forecasting','https://app.effectivealtruism.org/funds/far-future/payouts/6vDsjtUyDdvBa3sNeoNVvl','https://app.effectivealtruism.org/funds/far-future','Oliver Habryka|Alex Zhu|Matt Wage|Helen Toner|Matt Fallshaw',923150,0.0292,NULL,NULL,
  /* donation_process */ 'Donee submitted grant application through the application form for the April 2019 round of grants from the Long Term Future Fund, and was selected as a grant recipient (23 out of almost 100 applications were accepted)',
  /* intended_use_of_funds_category */ 'Living expenses during research project|Direct project expenses',
  /* intended_use_of_funds */ 'Grant to build a private platform where AI safety and policy researchers have direct access to a base of superforecaster-equivalents. Lagerros previously received two grants to work on the project: a half-time salary from Effective Altruism Grants, and a grant for direct project expenses from Berkeley Existential Risk Initiative.',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ 'Grant investigator and main influencer notes the same high-level reasons for the grant as for similar grants to Anothony Aguirre (Metaculus) and Ozzie Gooen (Foretold); the general reasons are explained in the grant writeup for Gooen. Habryka also mentions Lagerros being around the community for 3 years, and having done useful owrk and received other funding. Habryka mentions he did not assess the grant in detail; the main reason for granting from the Long Term Future Fund was due to logistical complications with other grantmakers (FHI and BERI), who already vouched for the value of the project',
  /* donor_amount_reason */ 'Likely to be the amount requested by the donee in the application (this is not stated explicitly by either the donor or the donee)',
  /* donor_timing_reason */ 'Timing determined by timing of grant round',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ 'The grant reasoning is written up by Oliver Habryka and is available at https://forum.effectivealtruism.org/posts/CJJDwgyqT4gXktq6g/long-term-future-fund-april-2019-grant-decisions The comments discuss this and the other forecasting grants, and include the question "why are you acting as grant-givers here rather than as special interest investors?" It is also included in a list of potentially concerning grants in a portfolio evaluation comment https://forum.effectivealtruism.org/posts/CJJDwgyqT4gXktq6g/long-term-future-fund-april-2019-grant-decisions#d4YHzSJnNWmyxf6HM by Evan Gaensbauer'),
  ('Effective Altruism Funds: Long-Term Future Fund','Orpheus Lummis',10000,'2019-04-07','day','donation log','AI safety/upskilling','https://app.effectivealtruism.org/funds/far-future/payouts/6vDsjtUyDdvBa3sNeoNVvl','https://app.effectivealtruism.org/funds/far-future','Oliver Habryka|Alex Zhu|Matt Wage|Helen Toner|Matt Fallshaw',923150,0.0108,NULL,NULL,
  /* donation_process */ 'Donee submitted grant application through the application form for the April 2019 round of grants from the Long Term Future Fund, and was selected as a grant recipient (23 out of almost 100 applications were accepted)',
  /* intended_use_of_funds_category */ 'Living expenses during research project',
  /* intended_use_of_funds */ 'Grant for upskilling in contemporary AI techniques, deep RL and AI safety, before pursuing a ML PhD. Notable planned subprojects: (1) Engaging with David Krueger’s AI safety reading group at Montreal Institute for Learning Algorithms (2) Starting & maintaining a public index of AI safety papers, to help future literature reviews and to complement https://vkrakovna.wordpress.com/ai-safety-resources/ as a standalone wiki-page (eg at http://aisafetyindex.net ) (3) From-scratch implementation of seminal deep RL algorithms (4) Going through textbooks: Goodfellow Bengio Courville 2016, Sutton Barto 2018 (5) Possibly doing the next AI Safety camp (6) Building a prioritization tool for English Wikipedia using NLP, building on the literature of quality assessment (https://paperpile.com/shared/BZ2jzQ) (7) Studying the AI Alignment literature',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ 'Grant investigator and main influencer Oliver Habryka is impressed with the results of the AI Safety Unconference organized by Lummis after NeurIPS with Long Term Future Fund money. However, he is not confident of the grant, writing: "I don’t know Orpheus very well, and while I have received generally positive reviews of their work, I haven’t yet had the time to look into any of those reviews in detail, and haven’t seen clear evidence about the quality of their judgment." Habryka also favors more time for self-study and reflection, and is excited about growing the Montral AI alignment community. Finally, Habryka thinks the grant amount is small and is unlikely to have negative consequences',
  /* donor_amount_reason */ 'Likely to be the amount requested by the donee in the application (this is not stated explicitly by either the donor or the donee). The small amount is also one reason grant investigator Oliver Habryka is comfortable making the grant despite not investigating thoroughly',
  /* donor_timing_reason */ 'Timing determined by timing of grant round',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ 'The grant reasoning is written up by Oliver Habryka and is available at https://forum.effectivealtruism.org/posts/CJJDwgyqT4gXktq6g/long-term-future-fund-april-2019-grant-decisions The comments on the post do not discuss this specific grant'),
  ('Effective Altruism Funds: Long-Term Future Fund','Lauren Lee',20000,'2019-04-07','day','donation log','Rationality community','https://app.effectivealtruism.org/funds/far-future/payouts/6vDsjtUyDdvBa3sNeoNVvl','https://app.effectivealtruism.org/funds/far-future','Oliver Habryka|Alex Zhu|Matt Wage|Helen Toner|Matt Fallshaw',923150,0.0217,NULL,NULL,
  /* donation_process */ 'Donee submitted grant application through the application form for the April 2019 round of grants from the Long Term Future Fund, and was selected as a grant recipient (23 out of almost 100 applications were accepted)',
  /* intended_use_of_funds_category */ 'Living expenses during research project',
  /* intended_use_of_funds */ 'Grant for working to prevent burnout and boost productivity within the EA and X-risk communities. From the grant application: (1) Grant requested to spend the coming year thinking about rationality and testing new projects. (2) The goal is to help individuals and orgs in the x-risk community orient towards and achieve their goals. (A) Training the skill of dependability. (B) Thinking clearly about AI risk. (C) Reducing burnout. (3) Measurable outputs include programs with 1-on-1 sessions with individuals or orgs, X-risk orgs spending time/money on services, writings or talks, workshops with feedback forms, and improved personal effectiveness',
  /* intended_funding_timeframe_in_months */ 6,
  /* donor_donee_reason */ 'Grant investigator and main influencer Habryka describes his grant reasoning as follows: "In sum, this grant hopefully helps Lauren to recover from burning out, get the new rationality projects she is working on off the ground, potentially identify a good new niche for her to work in (alone or at an existing organization), and write up her ideas for the community."',
  /* donor_amout_reason */ 'Likely to be the amount requested by the donee in the application (this is not stated explicitly by either the donor or the donee)',
  /* donor_timing_reason */ 'Timing determined by timing of grant round',
  /* donor_next_donation_thoughts */ 'Grant investigator and main influencer Oliver Habrkya qualifies the likelihood of giving another grant as follows: "I think that she should probably aim to make whatever she does valuable enough that individuals and organizations in the community wish to pay her directly for her work. It’s unlikely that I would recommend renewing this grant for another 6 month period in the absence of a relatively exciting new research project/direction, and if Lauren were to reapply, I would want to have a much stronger sense that the projects she was working on were producing lots of value before I decided to recommend funding her again."',
  /* donor_retrospective */ NULL,
  /* notes */ ' The grant reasoning is written up by Oliver Habryka and is available at https://forum.effectivealtruism.org/posts/CJJDwgyqT4gXktq6g/long-term-future-fund-april-2019-grant-decisions   The grant receives criticism in the comments, including ''This is ridiculous, I''m sure she''s a great person but please don''t use the gift you received to provide sinecures to people "in the community"'''),  
  ('Effective Altruism Funds: Long-Term Future Fund','Kocherga',50000,'2019-04-07','day','donation log','Rationality community','https://app.effectivealtruism.org/funds/far-future/payouts/6vDsjtUyDdvBa3sNeoNVvl','https://app.effectivealtruism.org/funds/far-future','Oliver Habryka|Alex Zhu|Matt Wage|Helen Toner|Matt Fallshaw',923150,0.0542,'Russia','Vyacheslav Matyuhin',
  /* donation_process */ 'Donee submitted grant application through the application form for the April 2019 round of grants from the Long Term Future Fund, and was selected as a grant recipient (23 out of almost 100 applications were accepted)',
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant to Vyacheslav Matyuhin for Kocherga, an offline community hub for rationalists and EAs in Moscow. Kocherga''s concrete plans with the grant include: (1) Add 2 more people to the team. (2) Implement a new community-building strategy. (3) Improve the rationalty workshops.',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ 'Grant investigator and main influencer Oliver Habryka notes that the Russian rationality community has been successful, with projects such as https://lesswrong.ru (Russian translation of LessWrong sequences), kickstarter to distribute copies of HPMOR, and Kocherga, a financially self-sustaining anti-cafe in Moscow that hosts a variety of events for roughly 100 attendees per week. The grant reasoning references the LessWrong post https://www.lesswrong.com/posts/WmfapdnpFfHWzkdXY/rationalist-community-hub-in-moscow-3-years-retrospective by Kocherga. The grant is being made by the Long Term Future Fund because the EA Meta Fund decided not to make it',
  /* donor_amount_reason */ 'Likely to be the amount requested by the donee in the application (this is not stated explicitly by either the donor or the donee)',
  /* donor_timing_reason */ 'Timing determined by timing of grant round',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ ' The grant reasoning is written up by Oliver Habryka and is available at https://forum.effectivealtruism.org/posts/CJJDwgyqT4gXktq6g/long-term-future-fund-april-2019-grant-decisions'),
  ('Effective Altruism Funds: Long-Term Future Fund','Connor Flexman',20000,'2019-04-07','day','donation log','AI safety/forecasting','https://app.effectivealtruism.org/funds/far-future/payouts/6vDsjtUyDdvBa3sNeoNVvl','https://app.effectivealtruism.org/funds/far-future','Oliver Habryka|Alex Zhu|Matt Wage|Helen Toner|Matt Fallshaw',923150,0.0217,NULL,NULL,
  /* donation_process */ 'Donee submitted grant application through the application form for the April 2019 round of grants from the Long Term Future Fund, and was selected as a grant recipient (23 out of almost 100 applications were accepted)',
  /* intended_use_of_funds_category */ 'Living expenses during research project',
  /* intended_use_of_funds */ 'Grant to perform independent research in collaboration with John Salvatier',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ 'The grant was originally requested by John Salvatier (who is already funded by an EA Grant), as a grant to Salvatier to hire Flexman to help him. But Oliver Habryka (the primary person on whose recommendation the grant was made) ultimately decided to give the money to Flexman to give him more flexibility to switch if the work with Salvatier does not go well. Despite the reservations, Habryka considers significant negative consequences unlkely. Habryka also says: "I assign some significant probability that this grant can help Connor develop into an excellent generalist researcher of a type that I feel like EA is currently quite bottlenecked on." Habryka has two other reservations: potential conflict of interest because he lives in the same house as the recipient, and lack of concrete, externally verifiable evidence of competence',
  /* donor_amount_reason */ 'Likely to be the amount requested by the donee in the application (this is not stated explicitly by either the donor or the donee)',
  /* donor_timing_reason */ 'Timing determined by timing of grant round',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ 'The grant reasoning is written up by Oliver Habryka and is available at https://forum.effectivealtruism.org/posts/CJJDwgyqT4gXktq6g/long-term-future-fund-april-2019-grant-decisions Habryka was the primary person on whose recommendation the grant was made. Habryka replies to a comment giving ideas on what independent research Flexman might produce if he stops working with Salvatier'),
  ('Effective Altruism Funds: Long-Term Future Fund','Eli Tyre',30000,'2019-04-07','day','donation log','Rationality improvement','https://app.effectivealtruism.org/funds/far-future/payouts/6vDsjtUyDdvBa3sNeoNVvl','https://app.effectivealtruism.org/funds/far-future','Oliver Habryka|Alex Zhu|Matt Wage|Helen Toner|Matt Fallshaw',923150,0.0325,NULL,NULL,
  /* donation_process */ 'Donee submitted grant application through the application form for the April 2019 round of grants from the Long Term Future Fund, and was selected as a grant recipient (23 out of almost 100 applications were accepted)',
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant to support projects for rationality and community building interventions. Example projects: facilitating conversations between top people in AI alignment, organization advanced workshops on double crux, doing independent research projects such as https://www.lesswrong.com/posts/tj8QP2EFdP8p54z6i/historical-mathematicians-exhibit-a-birth-order-effect-too (evaluating burth order effects in mathematicians), providing new EAs and rationalists with advice and guidance on how to get traction on working on important problems, and helping John Salvatier develop techniques around skill transfer. Grant investigator and main influencer Oliver Habryka writes: "the goal of this grant is to allow [Eli Tyre] to take actions with greater leverage by hiring contractors, paying other community members for services, and paying for other varied expenses associated with his projects."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ 'Grant investigation and main influencer is excited about the projects Tyre is interested in working on, and writes: "Eli has worked on a large variety of interesting and valuable projects over the last few years, many of them too small to have much payment infrastructure, resulting in him doing a lot of work without appropriate compensation. I think his work has been a prime example of picking low-hanging fruit by using local information and solving problems that aren’t worth solving at scale, and I want him to have resources to continue working in this space."',
  /* donor_amount_reason */ 'Likely to be the amount requested by the donee in the application (this is not stated explicitly by either the donor or the donee)',
  /* donor_timing_reason */ 'Timing determined by timing of grant round',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ ' The grant reasoning is written up by Oliver Habryka and is available at https://forum.effectivealtruism.org/posts/CJJDwgyqT4gXktq6g/long-term-future-fund-april-2019-grant-decision');

